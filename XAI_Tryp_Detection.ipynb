{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b21a7f0-2d73-4a91-ad1f-18ec06c09ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from typing import Callable, List, Optional, Tuple\n",
    "import torch\n",
    "import ttach as tta\n",
    "import math\n",
    "from typing import Dict, List\n",
    "import cv2\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from scipy.ndimage import zoom\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "import torchvision\n",
    "from sklearn.decomposition import KernelPCA\n",
    "import os\n",
    "import shutil\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from PIL import Image\n",
    "from ultralytics.nn.tasks import attempt_load_weights\n",
    "from ultralytics.utils.ops import non_max_suppression, xywh2xyxy\n",
    "from torchvision.ops import box_iou\n",
    "\n",
    "class BaseCAM:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        target_layers: List[torch.nn.Module],\n",
    "        reshape_transform: Callable = None,\n",
    "        compute_input_gradient: bool = False,\n",
    "        uses_gradients: bool = True,\n",
    "        tta_transforms: Optional[tta.Compose] = None,\n",
    "        detach: bool = True,\n",
    "    ) -> None:\n",
    "        #print('BaseCAM_init')\n",
    "        self.model = model.eval()\n",
    "        self.target_layers = target_layers\n",
    "\n",
    "        # Use the same device as the model.\n",
    "        self.device = next(self.model.parameters()).device\n",
    "        self.reshape_transform = reshape_transform\n",
    "        self.compute_input_gradient = compute_input_gradient\n",
    "        self.uses_gradients = uses_gradients\n",
    "        if tta_transforms is None:\n",
    "            self.tta_transforms = tta.Compose(\n",
    "                [\n",
    "                    tta.HorizontalFlip(),\n",
    "                    tta.Multiply(factors=[0.9, 1, 1.1]),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            self.tta_transforms = tta_transforms\n",
    "\n",
    "        self.detach = detach\n",
    "        self.activations_and_grads = ActivationsAndGradients(self.model, target_layers, reshape_transform)\n",
    "        \n",
    "    \"\"\" Get a vector of weights for every channel in the target layer.\n",
    "        Methods that return weights channels,\n",
    "        will typically need to only implement this function. \"\"\"\n",
    "\n",
    "    def get_cam_weights(\n",
    "        self,\n",
    "        input_tensor: torch.Tensor,\n",
    "        target_layers: List[torch.nn.Module],\n",
    "        targets: List[torch.nn.Module],\n",
    "        activations: torch.Tensor,\n",
    "        grads: torch.Tensor,\n",
    "    ) -> np.ndarray:\n",
    "        raise Exception(\"Not Implemented\")\n",
    "\n",
    "    def get_cam_image(\n",
    "        self,\n",
    "        input_tensor: torch.Tensor,\n",
    "        target_layer: torch.nn.Module,\n",
    "        targets: List[torch.nn.Module],\n",
    "        activations: torch.Tensor,\n",
    "        grads: torch.Tensor,\n",
    "        eigen_smooth: bool = False,\n",
    "    ) -> np.ndarray:\n",
    "        weights = self.get_cam_weights(input_tensor, target_layer, targets, activations, grads)\n",
    "        if isinstance(activations, torch.Tensor):\n",
    "            activations = activations.cpu().detach().numpy()\n",
    "        # 2D conv\n",
    "        if len(activations.shape) == 4:\n",
    "            weighted_activations = weights[:, :, None, None] * activations\n",
    "        # 3D conv\n",
    "        elif len(activations.shape) == 5:\n",
    "            weighted_activations = weights[:, :, None, None, None] * activations\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid activation shape. Get {len(activations.shape)}.\")\n",
    "\n",
    "        if eigen_smooth:\n",
    "            cam = get_2d_projection(weighted_activations)\n",
    "        else:\n",
    "            cam = weighted_activations.sum(axis=1)\n",
    "        return cam\n",
    "\n",
    "    def forward(\n",
    "        self, input_tensor: torch.Tensor, targets: List[torch.nn.Module], eigen_smooth: bool = False\n",
    "    ) -> np.ndarray:\n",
    "        input_tensor = input_tensor.to(self.device)\n",
    "        \n",
    "        if self.compute_input_gradient:\n",
    "            input_tensor = torch.autograd.Variable(input_tensor, requires_grad=True)\n",
    "\n",
    "        self.outputs = outputs = self.activations_and_grads(input_tensor)\n",
    "        #print('BaseCAM_forward_after_activations_and_grads')\n",
    "        if targets is None:\n",
    "            target_categories = np.argmax(outputs.cpu().data.numpy(), axis=-1)\n",
    "            targets = [ClassifierOutputTarget(category) for category in target_categories]\n",
    "        if self.uses_gradients:\n",
    "            self.model.zero_grad()\n",
    "            loss = sum([target(output) for target, output in zip(targets, outputs)])\n",
    "            \n",
    "            if self.detach:\n",
    "                loss.backward(retain_graph=True)\n",
    "            else:\n",
    "                # keep the computational graph, create_graph = True is needed for hvp\n",
    "                torch.autograd.grad(loss, input_tensor, retain_graph = True, create_graph = True)\n",
    "            if 'hpu' in str(self.device):\n",
    "                self.__htcore.mark_step()\n",
    "                \n",
    "        cam_per_layer = self.compute_cam_per_layer(input_tensor, targets, eigen_smooth)\n",
    "        return self.aggregate_multi_layers(cam_per_layer)\n",
    "\n",
    "    def get_target_width_height(self, input_tensor: torch.Tensor) -> Tuple[int, int]:\n",
    "        if len(input_tensor.shape) == 4:\n",
    "            width, height = input_tensor.size(-1), input_tensor.size(-2)\n",
    "            return width, height\n",
    "        elif len(input_tensor.shape) == 5:\n",
    "            depth, width, height = input_tensor.size(-1), input_tensor.size(-2), input_tensor.size(-3)\n",
    "            return depth, width, height\n",
    "        else:\n",
    "            raise ValueError(\"Invalid input_tensor shape. Only 2D or 3D images are supported.\")\n",
    "\n",
    "    def compute_cam_per_layer(\n",
    "        self, input_tensor: torch.Tensor, targets: List[torch.nn.Module], eigen_smooth: bool\n",
    "    ) -> np.ndarray:\n",
    "        if self.detach:\n",
    "            activations_list = [a.cpu().data.numpy() for a in self.activations_and_grads.activations]\n",
    "            grads_list = [g.cpu().data.numpy() for g in self.activations_and_grads.gradients]\n",
    "        else:\n",
    "            activations_list = [a for a in self.activations_and_grads.activations]\n",
    "            grads_list = [g for g in self.activations_and_grads.gradients]\n",
    "        target_size = self.get_target_width_height(input_tensor)\n",
    "\n",
    "        cam_per_target_layer = []\n",
    "        # Loop over the saliency image from every layer\n",
    "        for i in range(len(self.target_layers)):\n",
    "            target_layer = self.target_layers[i]\n",
    "            layer_activations = None\n",
    "            layer_grads = None\n",
    "            if i < len(activations_list):\n",
    "                layer_activations = activations_list[i]\n",
    "            if i < len(grads_list):\n",
    "                layer_grads = grads_list[i]\n",
    "\n",
    "            cam = self.get_cam_image(input_tensor, target_layer, targets, layer_activations, layer_grads, eigen_smooth)\n",
    "            cam = np.maximum(cam, 0)\n",
    "            scaled = scale_cam_image(cam, target_size)\n",
    "            cam_per_target_layer.append(scaled[:, None, :])\n",
    "\n",
    "        return cam_per_target_layer\n",
    "\n",
    "    def aggregate_multi_layers(self, cam_per_target_layer: np.ndarray) -> np.ndarray:\n",
    "        cam_per_target_layer = np.concatenate(cam_per_target_layer, axis=1)\n",
    "        cam_per_target_layer = np.maximum(cam_per_target_layer, 0)\n",
    "        result = np.mean(cam_per_target_layer, axis=1)\n",
    "        return scale_cam_image(result)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        input_tensor: torch.Tensor,\n",
    "        targets: List[torch.nn.Module] = None,\n",
    "        aug_smooth: bool = False,\n",
    "        eigen_smooth: bool = False,\n",
    "    ) -> np.ndarray:\n",
    "        # Smooth the CAM result with test time augmentation\n",
    "        # if aug_smooth is True:\n",
    "        #     return self.forward_augmentation_smoothing(input_tensor, targets, eigen_smooth)\n",
    "\n",
    "        return self.forward(input_tensor, targets, eigen_smooth)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.activations_and_grads.release()\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, exc_tb):\n",
    "        self.activations_and_grads.release()\n",
    "        if isinstance(exc_value, IndexError):\n",
    "            # Handle IndexError here...\n",
    "            print(f\"An exception occurred in CAM with block: {exc_type}. Message: {exc_value}\")\n",
    "            return True\n",
    "\n",
    "class ActivationsAndGradients:\n",
    "    \"\"\" Class for extracting activations and\n",
    "    registering gradients from targetted intermediate layers \"\"\"\n",
    "\n",
    "    def __init__(self, model: torch.nn.Module,\n",
    "                 target_layers: List[torch.nn.Module],\n",
    "                 reshape_transform: Optional[callable]) -> None:  # type: ignore\n",
    "        \"\"\"\n",
    "        Initializes the ActivationsAndGradients object.\n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module): The neural network model.\n",
    "            target_layers (List[torch.nn.Module]): List of target layers from which to extract activations and gradients.\n",
    "            reshape_transform (Optional[callable]): A function to transform the shape of the activations and gradients if needed.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.gradients = []\n",
    "        self.activations = []\n",
    "        self.reshape_transform = reshape_transform\n",
    "        self.handles = []\n",
    "        #print('ActivationsAndGradients_init')\n",
    "        for target_layer in target_layers:\n",
    "            self.handles.append(\n",
    "                target_layer.register_forward_hook(self.save_activation))\n",
    "            # Because of https://github.com/pytorch/pytorch/issues/61519,\n",
    "            # we don't use backward hook to record gradients.\n",
    "            self.handles.append(\n",
    "                target_layer.register_forward_hook(self.save_gradient))\n",
    "\n",
    "    def save_activation(self, module: torch.nn.Module,\n",
    "                        input: Union[torch.Tensor, Tuple[torch.Tensor, ...]],\n",
    "                        output: torch.Tensor) -> None:\n",
    "        \"\"\"\n",
    "        Saves the activation of the targeted layer.\n",
    "\n",
    "        Args:\n",
    "            module (torch.nn.Module): The targeted layer module.\n",
    "            input (Union[torch.Tensor, Tuple[torch.Tensor, ...]]): The input to the targeted layer.\n",
    "            output (torch.Tensor): The output activation of the targeted layer.\n",
    "        \"\"\"\n",
    "        activation = output\n",
    "\n",
    "        if self.reshape_transform is not None:\n",
    "            activation = self.reshape_transform(activation)\n",
    "        self.activations.append(activation.cpu().detach())\n",
    "\n",
    "    def save_gradient(self, module: torch.nn.Module,\n",
    "                      input: Union[torch.Tensor, Tuple[torch.Tensor, ...]],\n",
    "                      output: torch.Tensor) -> None:\n",
    "        \"\"\"\n",
    "        Saves the gradient of the targeted layer.\n",
    "\n",
    "        Args:\n",
    "            module (torch.nn.Module): The targeted layer module.\n",
    "            input (Union[torch.Tensor, Tuple[torch.Tensor, ...]]): The input to the targeted layer.\n",
    "            output (torch.Tensor): The output activation of the targeted layer.\n",
    "        \"\"\"\n",
    "        if not hasattr(output, \"requires_grad\") or not output.requires_grad:\n",
    "            # You can only register hooks on tensor requires grad.\n",
    "            return\n",
    "\n",
    "        # Gradients are computed in reverse order\n",
    "        def _store_grad(grad: torch.Tensor) -> None:\n",
    "            if self.reshape_transform is not None:\n",
    "                grad = self.reshape_transform(grad)\n",
    "            self.gradients = [grad.cpu().detach()] + self.gradients\n",
    "\n",
    "        output.register_hook(_store_grad)\n",
    "\n",
    "    def post_process(self, result: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Post-processes the result.\n",
    "\n",
    "        Args:\n",
    "            result (torch.Tensor): The result tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor, np.ndarray]: A tuple containing the post-processed result.\n",
    "        \"\"\"\n",
    "        logits_ = result[:, 4:]\n",
    "        boxes_ = result[:, :4]\n",
    "        sorted, indices = torch.sort(logits_.max(1)[0], descending=True)\n",
    "        return torch.transpose(logits_[0], dim0=0, dim1=1)[indices[0]], torch.transpose(boxes_[0], dim0=0, dim1=1)[\n",
    "            indices[0]], xywh2xyxy(torch.transpose(boxes_[0], dim0=0, dim1=1)[indices[0]]).cpu().detach().numpy()\n",
    "\n",
    "    def __call__(self, x: torch.Tensor) -> List[List[Union[torch.Tensor, np.ndarray]]]:\n",
    "        \"\"\"\n",
    "        Calls the ActivationsAndGradients object.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            List[List[Union[torch.Tensor, np.ndarray]]]: A list containing activations and gradients.\n",
    "        \"\"\"\n",
    "        #print('ActivationsAndGradients_call')\n",
    "        self.gradients = []\n",
    "        self.activations = []\n",
    "        model_output = self.model(x)\n",
    "        post_result, pre_post_boxes, post_boxes = self.post_process(\n",
    "            model_output[0])\n",
    "        return [[post_result, pre_post_boxes]]\n",
    "\n",
    "    def release(self) -> None:\n",
    "        \"\"\"Removes hooks.\"\"\"\n",
    "        for handle in self.handles:\n",
    "            handle.remove()\n",
    "\n",
    "class ScoreCAM(BaseCAM):\n",
    "    def __init__(\n",
    "            self,\n",
    "            model,\n",
    "            target_layers,\n",
    "            reshape_transform=None):\n",
    "        super(ScoreCAM, self).__init__(model,\n",
    "                                       target_layers,\n",
    "                                       reshape_transform=reshape_transform,\n",
    "                                       uses_gradients=False)\n",
    "\n",
    "    def get_cam_weights(self,\n",
    "                        input_tensor,\n",
    "                        target_layer,\n",
    "                        targets,\n",
    "                        activations,\n",
    "                        grads):\n",
    "        with torch.no_grad():\n",
    "            upsample = torch.nn.UpsamplingBilinear2d(\n",
    "                size=input_tensor.shape[-2:])\n",
    "            activation_tensor = torch.from_numpy(activations)\n",
    "            activation_tensor = activation_tensor.to(self.device)\n",
    "\n",
    "            upsampled = upsample(activation_tensor)\n",
    "\n",
    "            maxs = upsampled.view(upsampled.size(0),\n",
    "                                  upsampled.size(1), -1).max(dim=-1)[0]\n",
    "            mins = upsampled.view(upsampled.size(0),\n",
    "                                  upsampled.size(1), -1).min(dim=-1)[0]\n",
    "\n",
    "            maxs, mins = maxs[:, :, None, None], mins[:, :, None, None]\n",
    "            upsampled = (upsampled - mins) / (maxs - mins + 1e-8)\n",
    "\n",
    "            input_tensors = input_tensor[:, None,\n",
    "                                         :, :] * upsampled[:, :, None, :, :]\n",
    "\n",
    "            if hasattr(self, \"batch_size\"):\n",
    "                BATCH_SIZE = self.batch_size\n",
    "            else:\n",
    "                BATCH_SIZE = 16\n",
    "\n",
    "            scores = []\n",
    "            for target, tensor in zip(targets, input_tensors):\n",
    "                for i in tqdm.tqdm(range(0, tensor.size(0), BATCH_SIZE)):\n",
    "                    batch = tensor[i: i + BATCH_SIZE, :]\n",
    "                    # outputs = [target(o).cpu().item()\n",
    "                    #            for o in self.model(batch)]\n",
    "                    # model_out = self.model(batch)\n",
    "                    # print(f\"Model output type: {type(model_out)}\")\n",
    "                    # print(f\"Model output: {model_out}\")\n",
    "                    # outputs = [target((model_out, None)).cpu().item()]\n",
    "                    # model_output = self.model(batch)\n",
    "                    # outputs = [target(model_output).cpu().item()]\n",
    "\n",
    "                    model_output = self.model(batch)\n",
    "                    post_result, pre_post_boxes = model_output\n",
    "                    \n",
    "                    # Create a dummy tensor that supports [i,j] indexing for the target function\n",
    "                    # Since we just need it to work with the target function's indexing\n",
    "                    batch_size = post_result.shape[0]\n",
    "                    dummy_boxes = torch.zeros(batch_size, 4, device=post_result.device, dtype=post_result.dtype)\n",
    "                    \n",
    "                    # Reconstruct the tuple with compatible format\n",
    "                    formatted_output = (post_result, dummy_boxes)\n",
    "                    outputs = [target(formatted_output).cpu().item()]                    \n",
    "                    scores.extend(outputs)\n",
    "            scores = torch.Tensor(scores)\n",
    "            scores = scores.view(activations.shape[0], activations.shape[1])\n",
    "            weights = torch.nn.Softmax(dim=-1)(scores).numpy()\n",
    "            return weights\n",
    "\n",
    "class GradCAM(BaseCAM):\n",
    "    def __init__(self, model, target_layers,\n",
    "                 reshape_transform=None):\n",
    "        super(\n",
    "            GradCAM,\n",
    "            self).__init__(\n",
    "            model,\n",
    "            target_layers,\n",
    "            reshape_transform)\n",
    "        #print('GradCAM_init')\n",
    "    def get_cam_weights(self,\n",
    "                        input_tensor,\n",
    "                        target_layer,\n",
    "                        target_category,\n",
    "                        activations,\n",
    "                        grads):\n",
    "        # 2D image\n",
    "        if len(grads.shape) == 4:\n",
    "            return np.mean(grads, axis=(2, 3))\n",
    "        \n",
    "        # 3D image\n",
    "        elif len(grads.shape) == 5:\n",
    "            return np.mean(grads, axis=(2, 3, 4))\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Invalid grads shape.\" \n",
    "                             \"Shape of grads should be 4 (2D image) or 5 (3D image).\")\n",
    "\n",
    "class EigenCAM(BaseCAM):\n",
    "    def __init__(self, model, target_layers, \n",
    "                 reshape_transform=None):\n",
    "        super(EigenCAM, self).__init__(model,\n",
    "                                       target_layers,\n",
    "                                       reshape_transform,\n",
    "                                       uses_gradients=False)\n",
    "\n",
    "    def get_cam_image(self,\n",
    "                      input_tensor,\n",
    "                      target_layer,\n",
    "                      target_category,\n",
    "                      activations,\n",
    "                      grads,\n",
    "                      eigen_smooth):\n",
    "        return get_2d_projection(activations)\n",
    "\n",
    "class EigenGradCAM(BaseCAM):\n",
    "    def __init__(self, model, target_layers, \n",
    "                 reshape_transform=None):\n",
    "        super(EigenGradCAM, self).__init__(model, target_layers,\n",
    "                                           reshape_transform)\n",
    "\n",
    "    def get_cam_image(self,\n",
    "                      input_tensor,\n",
    "                      target_layer,\n",
    "                      target_category,\n",
    "                      activations,\n",
    "                      grads,\n",
    "                      eigen_smooth):\n",
    "        return get_2d_projection(grads * activations)\n",
    "\n",
    "\n",
    "class XGradCAM(BaseCAM):\n",
    "    def __init__(\n",
    "            self,\n",
    "            model,\n",
    "            target_layers,\n",
    "            reshape_transform=None):\n",
    "        super(\n",
    "            XGradCAM,\n",
    "            self).__init__(\n",
    "            model,\n",
    "            target_layers,\n",
    "            reshape_transform)\n",
    "\n",
    "    def get_cam_weights(self,\n",
    "                        input_tensor,\n",
    "                        target_layer,\n",
    "                        target_category,\n",
    "                        activations,\n",
    "                        grads):\n",
    "        sum_activations = np.sum(activations, axis=(2, 3))\n",
    "        eps = 1e-7\n",
    "        weights = grads * activations / \\\n",
    "            (sum_activations[:, :, None, None] + eps)\n",
    "        weights = weights.sum(axis=(2, 3))\n",
    "        return weights\n",
    "\n",
    "\n",
    "class RandomCAM(BaseCAM):\n",
    "    def __init__(self, model, target_layers, \n",
    "                 reshape_transform=None):\n",
    "        super(\n",
    "            RandomCAM,\n",
    "            self).__init__(\n",
    "            model,\n",
    "            target_layers,\n",
    "            reshape_transform)\n",
    "\n",
    "    def get_cam_weights(self,\n",
    "                        input_tensor,\n",
    "                        target_layer,\n",
    "                        target_category,\n",
    "                        activations,\n",
    "                        grads):\n",
    "        return np.random.uniform(-1, 1, size=(grads.shape[0], grads.shape[1]))\n",
    "        \n",
    "class LayerCAM(BaseCAM):\n",
    "    def __init__(\n",
    "            self,\n",
    "            model,\n",
    "            target_layers,\n",
    "            reshape_transform=None):\n",
    "        super(\n",
    "            LayerCAM,\n",
    "            self).__init__(\n",
    "            model,\n",
    "            target_layers,\n",
    "            reshape_transform)\n",
    "\n",
    "    def get_cam_image(self,\n",
    "                      input_tensor,\n",
    "                      target_layer,\n",
    "                      target_category,\n",
    "                      activations,\n",
    "                      grads,\n",
    "                      eigen_smooth):\n",
    "        spatial_weighted_activations = np.maximum(grads, 0) * activations\n",
    "\n",
    "        if eigen_smooth:\n",
    "            cam = get_2d_projection(spatial_weighted_activations)\n",
    "        else:\n",
    "            cam = spatial_weighted_activations.sum(axis=1)\n",
    "        return cam\n",
    "\n",
    "class KPCA_CAM(BaseCAM):\n",
    "    def __init__(self, model, target_layers, \n",
    "                 reshape_transform=None, kernel='sigmoid', gamma=None):\n",
    "        super(KPCA_CAM, self).__init__(model,\n",
    "                                       target_layers,\n",
    "                                       reshape_transform,\n",
    "                                       uses_gradients=False)\n",
    "        self.kernel=kernel\n",
    "        self.gamma=gamma\n",
    "\n",
    "    def get_cam_image(self,\n",
    "                      input_tensor,\n",
    "                      target_layer,\n",
    "                      target_category,\n",
    "                      activations,\n",
    "                      grads,\n",
    "                      eigen_smooth):\n",
    "        return get_2d_projection_kernel(activations, self.kernel, self.gamma)\n",
    "\n",
    "class HiResCAM(BaseCAM):\n",
    "    def __init__(self, model, target_layers, \n",
    "                 reshape_transform=None):\n",
    "        super(\n",
    "            HiResCAM,\n",
    "            self).__init__(\n",
    "            model,\n",
    "            target_layers,\n",
    "            reshape_transform)\n",
    "\n",
    "    def get_cam_image(self,\n",
    "                      input_tensor,\n",
    "                      target_layer,\n",
    "                      target_category,\n",
    "                      activations,\n",
    "                      grads,\n",
    "                      eigen_smooth):\n",
    "        elementwise_activations = grads * activations\n",
    "\n",
    "        if eigen_smooth:\n",
    "            print(\n",
    "                \"Warning: HiResCAM's faithfulness guarantees do not hold if smoothing is applied\")\n",
    "            cam = get_2d_projection(elementwise_activations)\n",
    "        else:\n",
    "            cam = elementwise_activations.sum(axis=1)\n",
    "        return cam\n",
    "\n",
    "class GradCAMPlusPlus(BaseCAM):\n",
    "    def __init__(self, model, target_layers,\n",
    "                 reshape_transform=None):\n",
    "        super(GradCAMPlusPlus, self).__init__(model, target_layers,\n",
    "                                              reshape_transform)\n",
    "\n",
    "    def get_cam_weights(self,\n",
    "                        input_tensor,\n",
    "                        target_layers,\n",
    "                        target_category,\n",
    "                        activations,\n",
    "                        grads):\n",
    "        grads_power_2 = grads**2\n",
    "        grads_power_3 = grads_power_2 * grads\n",
    "        # Equation 19 in https://arxiv.org/abs/1710.11063\n",
    "        sum_activations = np.sum(activations, axis=(2, 3))\n",
    "        eps = 0.000001\n",
    "        aij = grads_power_2 / (2 * grads_power_2 +\n",
    "                               sum_activations[:, :, None, None] * grads_power_3 + eps)\n",
    "        # Now bring back the ReLU from eq.7 in the paper,\n",
    "        # And zero out aijs where the activations are 0\n",
    "        aij = np.where(grads != 0, aij, 0)\n",
    "\n",
    "        weights = np.maximum(grads, 0) * aij\n",
    "        weights = np.sum(weights, axis=(2, 3))\n",
    "        return weights\n",
    "\n",
    "class GradCAMElementWise(BaseCAM):\n",
    "    def __init__(self, model, target_layers, \n",
    "                 reshape_transform=None):\n",
    "        super(\n",
    "            GradCAMElementWise,\n",
    "            self).__init__(\n",
    "            model,\n",
    "            target_layers,\n",
    "            reshape_transform)\n",
    "\n",
    "    def get_cam_image(self,\n",
    "                      input_tensor,\n",
    "                      target_layer,\n",
    "                      target_category,\n",
    "                      activations,\n",
    "                      grads,\n",
    "                      eigen_smooth):\n",
    "        elementwise_activations = np.maximum(grads * activations, 0)\n",
    "\n",
    "        if eigen_smooth:\n",
    "            cam = get_2d_projection(elementwise_activations)\n",
    "        else:\n",
    "            cam = elementwise_activations.sum(axis=1)\n",
    "        return cam\n",
    "        \n",
    "class FEM(BaseCAM):\n",
    "    def __init__(self, model, target_layers, \n",
    "                 reshape_transform=None, k=2):\n",
    "        super(FEM, self).__init__(model,\n",
    "                                       target_layers,\n",
    "                                       reshape_transform,\n",
    "                                       uses_gradients=False)\n",
    "        self.k = k\n",
    "\n",
    "    def get_cam_image(self,\n",
    "                      input_tensor,\n",
    "                      target_layer,\n",
    "                      target_category,\n",
    "                      activations,\n",
    "                      grads,\n",
    "                      eigen_smooth):\n",
    "        \n",
    "        \n",
    "        # 2D image\n",
    "        if len(activations.shape) == 4:\n",
    "            axis = (2, 3)\n",
    "        # 3D image\n",
    "        elif len(activations.shape) == 5:\n",
    "            axis = (2, 3, 4)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid activations shape.\" \n",
    "                             \"Shape of activations should be 4 (2D image) or 5 (3D image).\")\n",
    "        means = np.mean(activations, axis=axis)\n",
    "        stds = np.std(activations, axis=axis)\n",
    "        # k sigma rule:\n",
    "        # Add extra dimensions to match activations shape\n",
    "        th = means + self.k * stds\n",
    "        weights_shape = list(means.shape) + [1] * len(axis)\n",
    "        th = th.reshape(weights_shape)\n",
    "        binary_mask = activations > th\n",
    "        weights = binary_mask.mean(axis=axis)\n",
    "        return (weights.reshape(weights_shape) * activations).sum(axis=1)\n",
    "\n",
    "class FinerCAM:\n",
    "    def __init__(self, model: torch.nn.Module, target_layers: List[torch.nn.Module], reshape_transform: Callable = None, base_method=GradCAM):\n",
    "        self.base_cam = base_method(model, target_layers, reshape_transform)\n",
    "        self.compute_input_gradient = self.base_cam.compute_input_gradient\n",
    "        self.uses_gradients = self.base_cam.uses_gradients\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, targets: List[torch.nn.Module] = None, eigen_smooth: bool = False,\n",
    "                alpha: float = 1, comparison_categories: List[int] = [1, 2, 3], target_idx: int = None\n",
    "                ) -> np.ndarray:\n",
    "        input_tensor = input_tensor.to(self.base_cam.device)\n",
    "\n",
    "        if self.compute_input_gradient:\n",
    "            input_tensor = torch.autograd.Variable(input_tensor, requires_grad=True)\n",
    "\n",
    "        outputs = self.base_cam.activations_and_grads(input_tensor)\n",
    "\n",
    "        if targets is None:\n",
    "            output_data = outputs.detach().cpu().numpy()\n",
    "            target_logits = np.max(output_data, axis=-1) if target_idx is None else output_data[:, target_idx]\n",
    "            # Sort class indices for each sample based on the absolute difference \n",
    "            # between the class scores and the target logit, in ascending order.\n",
    "            # The most similar classes (smallest difference) appear first.\n",
    "            sorted_indices = np.argsort(np.abs(output_data - target_logits[:, None]), axis=-1)\n",
    "            targets = [FinerWeightedTarget(int(sorted_indices[i, 0]), \n",
    "                                           [int(sorted_indices[i, idx]) for idx in comparison_categories], \n",
    "                                           alpha) \n",
    "                       for i in range(output_data.shape[0])]\n",
    "\n",
    "        if self.uses_gradients:\n",
    "            self.base_cam.model.zero_grad()\n",
    "            loss = sum([target(output) for target, output in zip(targets, outputs)])\n",
    "            if self.base_cam.detach:\n",
    "                loss.backward(retain_graph=True)\n",
    "            else:\n",
    "                # keep the computational graph, create_graph = True is needed for hvp\n",
    "                torch.autograd.grad(loss, input_tensor, retain_graph = True, create_graph = True)\n",
    "                # When using the following loss.backward() method, a warning is raised: \"UserWarning: Using backward() with create_graph=True will create a reference cycle\"\n",
    "                # loss.backward(retain_graph=True, create_graph=True)\n",
    "            if 'hpu' in str(self.base_cam.device):\n",
    "                self.base_cam.__htcore.mark_step()\n",
    "\n",
    "        cam_per_layer = self.base_cam.compute_cam_per_layer(input_tensor, targets, eigen_smooth)\n",
    "        return self.base_cam.aggregate_multi_layers(cam_per_layer)\n",
    "\n",
    "\n",
    "def get_2d_projection(activation_batch):\n",
    "    # TBD: use pytorch batch svd implementation\n",
    "    activation_batch[np.isnan(activation_batch)] = 0\n",
    "    projections = []\n",
    "    for activations in activation_batch:\n",
    "        reshaped_activations = (activations).reshape(\n",
    "            activations.shape[0], -1).transpose()\n",
    "        # Centering before the SVD seems to be important here,\n",
    "        # Otherwise the image returned is negative\n",
    "        reshaped_activations = reshaped_activations - \\\n",
    "            reshaped_activations.mean(axis=0)\n",
    "        U, S, VT = np.linalg.svd(reshaped_activations, full_matrices=True)\n",
    "        projection = reshaped_activations @ VT[0, :]\n",
    "        projection = projection.reshape(activations.shape[1:])\n",
    "        projections.append(projection)\n",
    "    return np.float32(projections)\n",
    "\n",
    "\n",
    "\n",
    "def get_2d_projection_kernel(activation_batch, kernel='sigmoid', gamma=None):\n",
    "    activation_batch[np.isnan(activation_batch)] = 0\n",
    "    projections = []\n",
    "    for activations in activation_batch:\n",
    "        reshaped_activations = activations.reshape(activations.shape[0], -1).transpose()\n",
    "        reshaped_activations = reshaped_activations - reshaped_activations.mean(axis=0)\n",
    "        # Apply Kernel PCA\n",
    "        kpca = KernelPCA(n_components=1, kernel=kernel, gamma=gamma)\n",
    "        projection = kpca.fit_transform(reshaped_activations)\n",
    "        projection = projection.reshape(activations.shape[1:])\n",
    "        projections.append(projection)\n",
    "    return np.float32(projections)\n",
    "    \n",
    "class ClassifierOutputTarget:\n",
    "    def __init__(self, category):\n",
    "        self.category = category\n",
    "\n",
    "    def __call__(self, model_output):\n",
    "        if len(model_output.shape) == 1:\n",
    "            return model_output[self.category]\n",
    "        return model_output[:, self.category]\n",
    "\n",
    "\n",
    "class ClassifierOutputSoftmaxTarget:\n",
    "    def __init__(self, category):\n",
    "        self.category = category\n",
    "\n",
    "    def __call__(self, model_output):\n",
    "        if len(model_output.shape) == 1:\n",
    "            return torch.softmax(model_output, dim=-1)[self.category]\n",
    "        return torch.softmax(model_output, dim=-1)[:, self.category]\n",
    "\n",
    "\n",
    "class ClassifierOutputReST:\n",
    "    \"\"\"\n",
    "    Using both pre-softmax and post-softmax, proposed in https://arxiv.org/abs/2501.06261\n",
    "    \"\"\"\n",
    "    def __init__(self, category):\n",
    "        self.category = category\n",
    "    def __call__(self, model_output): \n",
    "        if len(model_output.shape) == 1:\n",
    "            target = torch.tensor([self.category], device=model_output.device)\n",
    "            model_output = model_output.unsqueeze(0)\n",
    "            return model_output[0][self.category] - torch.nn.functional.cross_entropy(model_output, target)\n",
    "        else:\n",
    "            target = torch.tensor([self.category] * model_output.shape[0], device=model_output.device)\n",
    "            return model_output[:,self.category] - torch.nn.functional.cross_entropy(model_output, target)\n",
    "\n",
    "\n",
    "class BinaryClassifierOutputTarget:\n",
    "    def __init__(self, category):\n",
    "        self.category = category\n",
    "\n",
    "    def __call__(self, model_output):\n",
    "        if self.category == 1:\n",
    "            sign = 1\n",
    "        else:\n",
    "            sign = -1\n",
    "        return model_output * sign\n",
    "\n",
    "\n",
    "class SoftmaxOutputTarget:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, model_output):\n",
    "        return torch.softmax(model_output, dim=-1)\n",
    "\n",
    "\n",
    "class RawScoresOutputTarget:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, model_output):\n",
    "        return model_output\n",
    "\n",
    "\n",
    "class SemanticSegmentationTarget:\n",
    "    \"\"\" Gets a binary spatial mask and a category,\n",
    "        And return the sum of the category scores,\n",
    "        of the pixels in the mask. \"\"\"\n",
    "\n",
    "    def __init__(self, category, mask):\n",
    "        self.category = category\n",
    "        self.mask = torch.from_numpy(mask)\n",
    "\n",
    "    def __call__(self, model_output):\n",
    "        return (model_output[self.category, :, :] * self.mask.to(model_output.device)).sum()\n",
    "\n",
    "\n",
    "class FasterRCNNBoxScoreTarget:\n",
    "    \"\"\" For every original detected bounding box specified in \"bounding boxes\",\n",
    "        assign a score on how the current bounding boxes match it,\n",
    "            1. In IOU\n",
    "            2. In the classification score.\n",
    "        If there is not a large enough overlap, or the category changed,\n",
    "        assign a score of 0.\n",
    "\n",
    "        The total score is the sum of all the box scores.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, labels, bounding_boxes, iou_threshold=0.5):\n",
    "        self.labels = labels\n",
    "        self.bounding_boxes = bounding_boxes\n",
    "        self.iou_threshold = iou_threshold\n",
    "\n",
    "    def __call__(self, model_outputs):\n",
    "        output = torch.Tensor([0])\n",
    "        if torch.cuda.is_available():\n",
    "            output = output.cuda()\n",
    "        elif torch.backends.mps.is_available():\n",
    "            output = output.to(\"mps\")\n",
    "\n",
    "        if len(model_outputs[\"boxes\"]) == 0:\n",
    "            return output\n",
    "\n",
    "        for box, label in zip(self.bounding_boxes, self.labels):\n",
    "            box = torch.Tensor(box[None, :])\n",
    "            if torch.cuda.is_available():\n",
    "                box = box.cuda()\n",
    "            elif torch.backends.mps.is_available():\n",
    "                box = box.to(\"mps\")\n",
    "\n",
    "            ious = torchvision.ops.box_iou(box, model_outputs[\"boxes\"])\n",
    "            index = ious.argmax()\n",
    "            if ious[0, index] > self.iou_threshold and model_outputs[\"labels\"][index] == label:\n",
    "                score = ious[0, index] + model_outputs[\"scores\"][index]\n",
    "                output = output + score\n",
    "        return output\n",
    "\n",
    "class FinerWeightedTarget:\n",
    "    \"\"\"\n",
    "    Computes a weighted difference between a primary category and a set of comparison categories.\n",
    "    \n",
    "    This target calculates the difference between the score for the main category and each of the comparison categories.\n",
    "    It obtains a weight for each comparison category from the softmax probabilities of the model output and computes a \n",
    "    weighted difference scaled by a comparison strength factor alpha.\n",
    "    \"\"\"\n",
    "    def __init__(self, main_category, comparison_categories, alpha):\n",
    "        self.main_category = main_category\n",
    "        self.comparison_categories = comparison_categories\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def __call__(self, model_output):\n",
    "        select = lambda idx: model_output[idx] if model_output.ndim == 1 else model_output[..., idx]\n",
    "        \n",
    "        wn = select(self.main_category)\n",
    "\n",
    "        prob = torch.softmax(model_output, dim=-1)\n",
    "\n",
    "        weights = [prob[idx] if model_output.ndim == 1 else prob[..., idx] for idx in self.comparison_categories]\n",
    "        numerator = sum(w * (wn - self.alpha * select(idx)) for w, idx in zip(weights, self.comparison_categories))\n",
    "        denominator = sum(weights)\n",
    "\n",
    "        return numerator / (denominator + 1e-9) \n",
    "        \n",
    "\n",
    "def preprocess_image(\n",
    "    img: np.ndarray, mean=[\n",
    "        0.5, 0.5, 0.5], std=[\n",
    "            0.5, 0.5, 0.5]) -> torch.Tensor:\n",
    "    preprocessing = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "    return preprocessing(img.copy()).unsqueeze(0)\n",
    "\n",
    "\n",
    "def deprocess_image(img):\n",
    "    \"\"\" see https://github.com/jacobgil/keras-grad-cam/blob/master/grad-cam.py#L65 \"\"\"\n",
    "    img = img - np.mean(img)\n",
    "    img = img / (np.std(img) + 1e-5)\n",
    "    img = img * 0.1\n",
    "    img = img + 0.5\n",
    "    img = np.clip(img, 0, 1)\n",
    "    return np.uint8(img * 255)\n",
    "\n",
    "\n",
    "def show_cam_on_image(img: np.ndarray,\n",
    "                      mask: np.ndarray,\n",
    "                      use_rgb: bool = False,\n",
    "                      colormap: int = cv2.COLORMAP_JET,\n",
    "                      image_weight: float = 0.5) -> np.ndarray:\n",
    "    \"\"\" This function overlays the cam mask on the image as an heatmap.\n",
    "    By default the heatmap is in BGR format.\n",
    "\n",
    "    :param img: The base image in RGB or BGR format.\n",
    "    :param mask: The cam mask.\n",
    "    :param use_rgb: Whether to use an RGB or BGR heatmap, this should be set to True if 'img' is in RGB format.\n",
    "    :param colormap: The OpenCV colormap to be used.\n",
    "    :param image_weight: The final result is image_weight * img + (1-image_weight) * mask.\n",
    "    :returns: The default image with the cam overlay.\n",
    "    \"\"\"\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), colormap)\n",
    "    if use_rgb:\n",
    "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "\n",
    "    if np.max(img) > 1:\n",
    "        raise Exception(\n",
    "            \"The input image should np.float32 in the range [0, 1]\")\n",
    "\n",
    "    if image_weight < 0 or image_weight > 1:\n",
    "        raise Exception(\n",
    "            f\"image_weight should be in the range [0, 1].\\\n",
    "                Got: {image_weight}\")\n",
    "\n",
    "    cam = (1 - image_weight) * heatmap + image_weight * img\n",
    "    cam = cam / np.max(cam)\n",
    "    return np.uint8(255 * cam)\n",
    "\n",
    "\n",
    "def create_labels_legend(concept_scores: np.ndarray,\n",
    "                         labels: Dict[int, str],\n",
    "                         top_k=2):\n",
    "    concept_categories = np.argsort(concept_scores, axis=1)[:, ::-1][:, :top_k]\n",
    "    concept_labels_topk = []\n",
    "    for concept_index in range(concept_categories.shape[0]):\n",
    "        categories = concept_categories[concept_index, :]\n",
    "        concept_labels = []\n",
    "        for category in categories:\n",
    "            score = concept_scores[concept_index, category]\n",
    "            label = f\"{','.join(labels[category].split(',')[:3])}:{score:.2f}\"\n",
    "            concept_labels.append(label)\n",
    "        concept_labels_topk.append(\"\\n\".join(concept_labels))\n",
    "    return concept_labels_topk\n",
    "\n",
    "\n",
    "def show_factorization_on_image(img: np.ndarray,\n",
    "                                explanations: np.ndarray,\n",
    "                                colors: List[np.ndarray] = None,\n",
    "                                image_weight: float = 0.5,\n",
    "                                concept_labels: List = None) -> np.ndarray:\n",
    "    \"\"\" Color code the different component heatmaps on top of the image.\n",
    "        Every component color code will be magnified according to the heatmap itensity\n",
    "        (by modifying the V channel in the HSV color space),\n",
    "        and optionally create a lagend that shows the labels.\n",
    "\n",
    "        Since different factorization component heatmaps can overlap in principle,\n",
    "        we need a strategy to decide how to deal with the overlaps.\n",
    "        This keeps the component that has a higher value in it's heatmap.\n",
    "\n",
    "    :param img: The base image RGB format.\n",
    "    :param explanations: A tensor of shape num_componetns x height x width, with the component visualizations.\n",
    "    :param colors: List of R, G, B colors to be used for the components.\n",
    "                   If None, will use the gist_rainbow cmap as a default.\n",
    "    :param image_weight: The final result is image_weight * img + (1-image_weight) * visualization.\n",
    "    :concept_labels: A list of strings for every component. If this is paseed, a legend that shows\n",
    "                     the labels and their colors will be added to the image.\n",
    "    :returns: The visualized image.\n",
    "    \"\"\"\n",
    "    n_components = explanations.shape[0]\n",
    "    if colors is None:\n",
    "        # taken from https://github.com/edocollins/DFF/blob/master/utils.py\n",
    "        _cmap = plt.cm.get_cmap('gist_rainbow')\n",
    "        colors = [\n",
    "            np.array(\n",
    "                _cmap(i)) for i in np.arange(\n",
    "                0,\n",
    "                1,\n",
    "                1.0 /\n",
    "                n_components)]\n",
    "    concept_per_pixel = explanations.argmax(axis=0)\n",
    "    masks = []\n",
    "    for i in range(n_components):\n",
    "        mask = np.zeros(shape=(img.shape[0], img.shape[1], 3))\n",
    "        mask[:, :, :] = colors[i][:3]\n",
    "        explanation = explanations[i]\n",
    "        explanation[concept_per_pixel != i] = 0\n",
    "        mask = np.uint8(mask * 255)\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_RGB2HSV)\n",
    "        mask[:, :, 2] = np.uint8(255 * explanation)\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_HSV2RGB)\n",
    "        mask = np.float32(mask) / 255\n",
    "        masks.append(mask)\n",
    "\n",
    "    mask = np.sum(np.float32(masks), axis=0)\n",
    "    result = img * image_weight + mask * (1 - image_weight)\n",
    "    result = np.uint8(result * 255)\n",
    "\n",
    "    if concept_labels is not None:\n",
    "        px = 1 / plt.rcParams['figure.dpi']  # pixel in inches\n",
    "        fig = plt.figure(figsize=(result.shape[1] * px, result.shape[0] * px))\n",
    "        plt.rcParams['legend.fontsize'] = int(\n",
    "            14 * result.shape[0] / 256 / max(1, n_components / 6))\n",
    "        lw = 5 * result.shape[0] / 256\n",
    "        lines = [Line2D([0], [0], color=colors[i], lw=lw)\n",
    "                 for i in range(n_components)]\n",
    "        plt.legend(lines,\n",
    "                   concept_labels,\n",
    "                   mode=\"expand\",\n",
    "                   fancybox=True,\n",
    "                   shadow=True)\n",
    "\n",
    "        plt.tight_layout(pad=0, w_pad=0, h_pad=0)\n",
    "        plt.axis('off')\n",
    "        fig.canvas.draw()\n",
    "        data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "        plt.close(fig=fig)\n",
    "        data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        data = cv2.resize(data, (result.shape[1], result.shape[0]))\n",
    "        result = np.hstack((result, data))\n",
    "    return result\n",
    "\n",
    "\n",
    "def scale_cam_image(cam, target_size=None):\n",
    "    result = []\n",
    "    for img in cam:\n",
    "        img = img - np.min(img)\n",
    "        img = img / (1e-7 + np.max(img))\n",
    "        if target_size is not None:\n",
    "            if len(img.shape) > 2:\n",
    "                img = zoom(np.float32(img), [\n",
    "                           (t_s / i_s) for i_s, t_s in zip(img.shape, target_size[::-1])])\n",
    "            else:\n",
    "                img = cv2.resize(np.float32(img), target_size)\n",
    "\n",
    "        result.append(img)\n",
    "    result = np.float32(result)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def scale_accross_batch_and_channels(tensor, target_size):\n",
    "    batch_size, channel_size = tensor.shape[:2]\n",
    "    reshaped_tensor = tensor.reshape(\n",
    "        batch_size * channel_size, *tensor.shape[2:])\n",
    "    result = scale_cam_image(reshaped_tensor, target_size)\n",
    "    result = result.reshape(\n",
    "        batch_size,\n",
    "        channel_size,\n",
    "        target_size[1],\n",
    "        target_size[0])\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def letterbox(\n",
    "    im: np.ndarray,\n",
    "    new_shape=(640, 640),\n",
    "    color=(114, 114, 114),\n",
    "    auto=True,\n",
    "    scaleFill=False,\n",
    "    scaleup=True,\n",
    "    stride=32,\n",
    "):\n",
    "    \"\"\"\n",
    "    Resize and pad image while meeting stride-multiple constraints.\n",
    "\n",
    "    Args:\n",
    "        im (numpy.ndarray): Input image.\n",
    "        new_shape (tuple, optional): Desired output shape. Defaults to (640, 640).\n",
    "        color (tuple, optional): Color of the border. Defaults to (114, 114, 114).\n",
    "        auto (bool, optional): Whether to automatically determine padding. Defaults to True.\n",
    "        scaleFill (bool, optional): Whether to stretch the image to fill the new shape. Defaults to False.\n",
    "        scaleup (bool, optional): Whether to scale the image up if necessary. Defaults to True.\n",
    "        stride (int, optional): Stride of the sliding window. Defaults to 32.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Letterboxed image.\n",
    "        tuple: Ratio of the resized image.\n",
    "        tuple: Padding sizes.\n",
    "\n",
    "    \"\"\"\n",
    "    shape = im.shape[:2]  # current shape [height, width]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    ratio = r, r  # width, height ratios\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "    if auto:  # minimum rectangle\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
    "    elif scaleFill:  # stretch\n",
    "        dw, dh = 0.0, 0.0\n",
    "        new_unpad = (new_shape[1], new_shape[0])\n",
    "        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "    \n",
    "    return im, ratio, (dw, dh)\n",
    "\n",
    "def display_images1(images):\n",
    "    \"\"\"\n",
    "    Display a list of PIL images in a grid.\n",
    "\n",
    "    Args:\n",
    "        images (list[PIL.Image]): A list of PIL images to display.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, len(images), figsize=(15, 7))\n",
    "    if len(images) == 1:\n",
    "        axes = [axes]\n",
    "    for ax, img in zip(axes, images):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def display_images(images, save_path=None):\n",
    "    \"\"\"\n",
    "    Display a list of PIL images in a grid.\n",
    "    Args:\n",
    "        images (list[PIL.Image]): A list of PIL images to display.\n",
    "        save_path (str, optional): Path to save the image grid.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, len(images), figsize=(15, 7))\n",
    "    if len(images) == 1:\n",
    "        axes = [axes]\n",
    "    for ax, img in zip(axes, images):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "def get_top_predictions(outputs, conf_threshold):\n",
    "    \"\"\"\n",
    "    Filter YOLOv8 predictions based on confidence threshold and return sum of top prediction boxes.\n",
    "    \n",
    "    Args:\n",
    "        outputs: Nested list [[post_result, pre_post_boxes]]\n",
    "        conf_threshold: Confidence threshold (e.g., 0.5)\n",
    "    \n",
    "    Returns:\n",
    "        Tensor with sum of box coordinates for top prediction, or None if no predictions above threshold\n",
    "    \"\"\"\n",
    "    # Extract tensors from nested structure\n",
    "    post_result, pre_post_boxes = outputs[0][0], outputs[0][1]\n",
    "    \n",
    "    # Get confidence values (squeeze to remove extra dimension)\n",
    "    confidences = post_result.squeeze()  # Shape: [6300]\n",
    "    \n",
    "    # Find indices where confidence >= threshold\n",
    "    valid_mask = confidences >= conf_threshold\n",
    "    valid_indices = torch.where(valid_mask)[0]\n",
    "    \n",
    "    if len(valid_indices) == 0:\n",
    "        return None  # No predictions above threshold\n",
    "    \n",
    "    # Get confidences and boxes for valid detections\n",
    "    valid_confidences = confidences[valid_indices]\n",
    "    valid_boxes = pre_post_boxes[valid_indices]  # Shape: [N, 4]\n",
    "    \n",
    "    # Find the index of the highest confidence prediction\n",
    "    top_idx = torch.argmax(valid_confidences)\n",
    "    \n",
    "    # Get the top prediction box and sum its coordinates\n",
    "    top_box = valid_boxes[top_idx]  # Shape: [4]\n",
    "    box_sum = torch.sum(top_box)  # Sum all 4 coordinates\n",
    "    \n",
    "    return box_sum\n",
    "    \n",
    "    # Get max confidence for each detection\n",
    "    max_confidences = post_result.max(dim=1)[0]  # Shape: [6300]\n",
    "    \n",
    "    # Find indices where confidence >= threshold\n",
    "    valid_indices = torch.where(max_confidences >= conf_threshold)[0]\n",
    "    \n",
    "    if len(valid_indices) == 0:\n",
    "        return None  # No predictions above threshold\n",
    "    \n",
    "    # Get confidences and boxes for valid detections\n",
    "    valid_confidences = max_confidences[valid_indices]\n",
    "    valid_boxes = pre_post_boxes[valid_indices]\n",
    "    \n",
    "    # Find the index of the highest confidence prediction\n",
    "    top_idx = torch.argmax(valid_confidences)\n",
    "    \n",
    "    # Get the actual index in the original arrays\n",
    "    original_idx = valid_indices[top_idx]\n",
    "    print(valid_confidences[top_idx].item())\n",
    "    return valid_confidences[top_idx].item()\n",
    "\n",
    "\n",
    "class GradCAM(BaseCAM):\n",
    "    def __init__(self, model, target_layers,\n",
    "                 reshape_transform=None):\n",
    "        super(\n",
    "            GradCAM,\n",
    "            self).__init__(\n",
    "            model,\n",
    "            target_layers,\n",
    "            reshape_transform)\n",
    "        #print('GradCAM_init')\n",
    "    def get_cam_weights(self,\n",
    "                        input_tensor,\n",
    "                        target_layer,\n",
    "                        target_category,\n",
    "                        activations,\n",
    "                        grads):\n",
    "        #print('GradCAM_get_cam_weights')\n",
    "        # 2D image\n",
    "        if len(grads.shape) == 4:\n",
    "            return np.mean(grads, axis=(2, 3))\n",
    "        \n",
    "        # 3D image\n",
    "        elif len(grads.shape) == 5:\n",
    "            return np.mean(grads, axis=(2, 3, 4))\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Invalid grads shape.\" \n",
    "                             \"Shape of grads should be 4 (2D image) or 5 (3D image).\")\n",
    "\n",
    "        \n",
    "\n",
    "class yolov8_heatmap:\n",
    "    \"\"\"\n",
    "    This class is used to implement the YOLOv8 target layer.\n",
    "\n",
    "     Args:\n",
    "            weight (str): The path to the checkpoint file.\n",
    "            device (str): The device to use for inference. Defaults to \"cuda:0\" if a GPU is available, otherwise \"cpu\".\n",
    "            method (str): The method to use for computing the CAM. Defaults to \"EigenGradCAM\".\n",
    "            layer (list): The indices of the layers to use for computing the CAM. Defaults to [10, 12, 14, 16, 18, -3].\n",
    "            conf_threshold (float): The confidence threshold for detections. Defaults to 0.2.\n",
    "            ratio (float): The ratio of maximum scores to return. Defaults to 0.02.\n",
    "            show_box (bool): Whether to show bounding boxes with the CAM. Defaults to True.\n",
    "            renormalize (bool): Whether to renormalize the CAM to be in the range [0, 1] across the entire image. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        A tensor containing the output.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            weight: str,\n",
    "            device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "            method=\"GradCAM\",\n",
    "            # layer=[12, 15, 17, 21],\n",
    "            layer=[17, 15],\n",
    "            conf_threshold=0.2,\n",
    "            ratio=0.02,\n",
    "            show_box=True,\n",
    "            renormalize=False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the YOLOv8 heatmap layer.\n",
    "        \"\"\"\n",
    "        device = device\n",
    "        backward_type = \"all\"\n",
    "        ckpt = torch.load(weight)\n",
    "        model_names = ckpt['model'].names\n",
    "        model = attempt_load_weights(weight, device)\n",
    "        model.info()\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad_(True)\n",
    "        model.eval()\n",
    "        #print('yolov8_heatmap_init')\n",
    "        target = yolov8_target(backward_type, conf_threshold, ratio)\n",
    "        target_layers = [model.model[l] for l in layer]\n",
    "        method = eval(method)(model, target_layers)\n",
    "        colors = np.random.uniform(\n",
    "            0, 255, size=(len(model_names), 3)).astype(int)\n",
    "        self.__dict__.update(locals())\n",
    "\n",
    "    def post_process(self, result):\n",
    "        \"\"\"\n",
    "        Perform non-maximum suppression on the detections and process results.\n",
    "\n",
    "        Args:\n",
    "            result (torch.Tensor): The raw detections from the model.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Filtered and processed detections.\n",
    "        \"\"\"\n",
    "        # Perform non-maximum suppression\n",
    "        processed_result = non_max_suppression(\n",
    "            result,\n",
    "            conf_thres=self.conf_threshold,  # Use the class's confidence threshold\n",
    "            iou_thres=0.5  # Intersection over Union threshold\n",
    "        )\n",
    "\n",
    "        # If no detections, return an empty tensor\n",
    "        if len(processed_result) == 0 or processed_result[0].numel() == 0:\n",
    "            return torch.empty(0, 6)  # Return an empty tensor with 6 columns\n",
    "\n",
    "        # Take the first batch of detections (assuming single image)\n",
    "        detections = processed_result[0]\n",
    "\n",
    "        # Filter detections based on confidence\n",
    "        mask = detections[:, 4] >= self.conf_threshold\n",
    "        filtered_detections = detections[mask]\n",
    "\n",
    "        return filtered_detections\n",
    "\n",
    "    def draw_detections(self, box, color, name, img):\n",
    "        \"\"\"\n",
    "        Draw bounding boxes and labels on an image for multiple detections.\n",
    "\n",
    "        Args:\n",
    "            box (torch.Tensor or np.ndarray): The bounding box coordinates in the format [x1, y1, x2, y2]\n",
    "            color (list): The color of the bounding box in the format [B, G, R]\n",
    "            name (str): The label for the bounding box.\n",
    "            img (np.ndarray): The image on which to draw the bounding box\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The image with the bounding box drawn.\n",
    "        \"\"\"\n",
    "        # Ensure box coordinates are integers\n",
    "        xmin, ymin, xmax, ymax = map(int, box[:4])\n",
    "\n",
    "        # Draw rectangle\n",
    "        cv2.rectangle(img, (xmin, ymin), (xmax, ymax),\n",
    "                      tuple(int(x) for x in color), 2)\n",
    "\n",
    "        # Draw label\n",
    "        cv2.putText(img, name, (xmin, ymin - 5),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.8, tuple(int(x) for x in color), 2,\n",
    "                    lineType=cv2.LINE_AA)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def renormalize_cam_in_bounding_boxes(\n",
    "            self,\n",
    "            boxes: np.ndarray,  # type: ignore\n",
    "            image_float_np: np.ndarray,  # type: ignore\n",
    "            grayscale_cam: np.ndarray,  # type: ignore\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Normalize the CAM to be in the range [0, 1]\n",
    "        inside every bounding boxes, and zero outside of the bounding boxes.\n",
    "\n",
    "        Args:\n",
    "            boxes (np.ndarray): The bounding boxes.\n",
    "            image_float_np (np.ndarray): The image as a numpy array of floats in the range [0, 1].\n",
    "            grayscale_cam (np.ndarray): The CAM as a numpy array of floats in the range [0, 1].\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The renormalized CAM.\n",
    "        \"\"\"\n",
    "        renormalized_cam = np.zeros(grayscale_cam.shape, dtype=np.float32)\n",
    "        for x1, y1, x2, y2 in boxes:\n",
    "            x1, y1 = max(x1, 0), max(y1, 0)\n",
    "            x2, y2 = min(grayscale_cam.shape[1] - 1,\n",
    "                         x2), min(grayscale_cam.shape[0] - 1, y2)\n",
    "            renormalized_cam[y1:y2, x1:x2] = scale_cam_image(\n",
    "                grayscale_cam[y1:y2, x1:x2].copy())\n",
    "        renormalized_cam = scale_cam_image(renormalized_cam)\n",
    "        eigencam_image_renormalized = show_cam_on_image(\n",
    "            image_float_np, renormalized_cam, use_rgb=True)\n",
    "        return eigencam_image_renormalized\n",
    "\n",
    "    def renormalize_cam(self, boxes, image_float_np, grayscale_cam):\n",
    "        \"\"\"Normalize the CAM to be in the range [0, 1]\n",
    "        across the entire image.\"\"\"\n",
    "        renormalized_cam = scale_cam_image(grayscale_cam)\n",
    "        eigencam_image_renormalized = show_cam_on_image(\n",
    "            image_float_np, renormalized_cam, use_rgb=True)\n",
    "        return eigencam_image_renormalized\n",
    "\n",
    "    def process(self, img_path):\n",
    "        \"\"\"Process the input image and generate CAM visualization.\"\"\"\n",
    "        img = cv2.imread(img_path)\n",
    "        img = letterbox(img)[0]\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = np.float32(img) / 255.0\n",
    "        tensor = (\n",
    "            torch.from_numpy(np.transpose(img, axes=[2, 0, 1]))\n",
    "            .unsqueeze(0)\n",
    "            .to(self.device)\n",
    "        )\n",
    "        #print('yolov8_heatmap_process')\n",
    "        try:\n",
    "            grayscale_cam = self.method(tensor, [self.target])\n",
    "        except AttributeError as e:\n",
    "            #print(e)\n",
    "            return\n",
    "\n",
    "        grayscale_cam = grayscale_cam[0, :]\n",
    "\n",
    "        pred1 = self.model(tensor)[0]\n",
    "        pred = non_max_suppression(\n",
    "            pred1,\n",
    "            conf_thres=self.conf_threshold,\n",
    "            iou_thres=0.45\n",
    "        )[0]\n",
    "        # print(pred)\n",
    "        # Debugging print\n",
    "\n",
    "        if self.renormalize:\n",
    "            cam_image = self.renormalize_cam(\n",
    "                pred[:, :4].cpu().detach().numpy().astype(np.int32),\n",
    "                img,\n",
    "                grayscale_cam\n",
    "            )\n",
    "        else:\n",
    "            cam_image = show_cam_on_image(img, grayscale_cam, use_rgb=True)\n",
    "            pure_cam = cam_image.copy()\n",
    "        if self.show_box and len(pred) > 0:\n",
    "            for detection in pred:\n",
    "                detection = detection.cpu().detach().numpy()\n",
    "\n",
    "                # Get class index and confidence\n",
    "                class_index = int(detection[5])\n",
    "                conf = detection[4]\n",
    "\n",
    "                # Draw detection\n",
    "                cam_image = self.draw_detections(\n",
    "                    detection[:4],  # Box coordinates\n",
    "                    self.colors[class_index],  # Color for this class\n",
    "                    f\"{self.model_names[class_index]}\",  # Label with confidence\n",
    "                    cam_image,\n",
    "                )\n",
    "\n",
    "        cam_image = Image.fromarray(cam_image)\n",
    "        pure_cam = Image.fromarray(pure_cam)\n",
    "        return cam_image, pure_cam, grayscale_cam, pred\n",
    "\n",
    "    def __call__(self, img_path):\n",
    "        \"\"\"\n",
    "        Generate CAM visualizations for one or more images.\n",
    "    \n",
    "        Args:\n",
    "            img_path (str): Path to the input image or directory containing images.\n",
    "    \n",
    "        Returns:\n",
    "            Tuple[List[PIL.Image], List[PIL.Image], List[np.ndarray], List[torch.Tensor]]:\n",
    "            (cam_with_boxes, cam_without_boxes, saliency_maps, bounding_boxes)\n",
    "        \"\"\"\n",
    "        cam_images = []\n",
    "        pure_cams = []\n",
    "        salient_maps = []\n",
    "        bounding_boxes = []\n",
    "        #print('yolov8_heatmap_call')\n",
    "        if os.path.isdir(img_path):\n",
    "            for img_path_ in os.listdir(img_path):\n",
    "                cam_img, pure_cam, salient_map, pred_boxes = self.process(f\"{img_path}/{img_path_}\")\n",
    "                cam_images.append(cam_img)\n",
    "                pure_cams.append(pure_cam)\n",
    "                salient_maps.append(salient_map)\n",
    "                bounding_boxes.append(pred_boxes)\n",
    "        else:\n",
    "            cam_img, pure_cam, salient_map, pred_boxes = self.process(img_path)\n",
    "            cam_images.append(cam_img)\n",
    "            pure_cams.append(pure_cam)\n",
    "            salient_maps.append(salient_map)\n",
    "            bounding_boxes.append(pred_boxes)\n",
    "    \n",
    "        return cam_images, pure_cams, salient_maps, bounding_boxes\n",
    "    \n",
    "\n",
    "class yolov8_target(torch.nn.Module):\n",
    "    def __init__(self, ouput_type, conf, ratio) -> None:\n",
    "        super().__init__()\n",
    "        self.ouput_type = ouput_type\n",
    "        self.conf = conf\n",
    "        self.ratio = ratio\n",
    "        #print('yolov8_target_init')\n",
    "\n",
    "    def forward(self, data):\n",
    "        #print('yolov8_target_forward')\n",
    "        print(f\"Data type: {type(data)}\")\n",
    "        print(f\"Data length: {len(data)}\")\n",
    "        \n",
    "        post_result, pre_post_boxes = data\n",
    "        \n",
    "        print(f\"post_result type: {type(post_result)}\")\n",
    "        print(f\"post_result shape: {getattr(post_result, 'shape', 'no shape')}\")\n",
    "        print(f\"pre_post_boxes type: {type(pre_post_boxes)}\")\n",
    "        print(f\"pre_post_boxes shape/length: {getattr(pre_post_boxes, 'shape', len(pre_post_boxes) if hasattr(pre_post_boxes, '__len__') else 'no length')}\")\n",
    "        \n",
    "        result = []\n",
    "        for i in range(post_result.size(0)):\n",
    "            if float(post_result[i].max()) >= self.conf:\n",
    "                if self.ouput_type == 'class' or self.ouput_type == 'all':\n",
    "                    result.append(post_result[i].max())\n",
    "                if self.ouput_type == 'box' or self.ouput_type == 'all':\n",
    "                    for j in range(4):\n",
    "                        result.append(pre_post_boxes[i, j])\n",
    "        return sum(result)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1652e50-97f3-4502-bb62-cb5518173dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    methods = [\"LayerCAM\"]\n",
    "    # methods = [\"EigenGradCAM\", \"GradCAM\", \"EigenCAM\", \"XGradCAM\", \"RandomCAM\", \"LayerCAM\", \"KPCA_CAM\", \"HiResCAM\", \"GradCAMPlusPlus\", \"GradCAMElementWise\"]\n",
    "    image_folder = '/home/axy9651/Tryp/trypanosome_parasite_detection/chagas_detection/DINO-atten-spa/data_cvat/val2017/images/'\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif']\n",
    "    layers =[[15]]\n",
    "    for method in methods:\n",
    "        print(method)\n",
    "        for layer in layers:\n",
    "            model = yolov8_heatmap(\n",
    "                weight=\"best_chagas.pt\",\n",
    "                method=method,\n",
    "                layer=layer\n",
    "            )\n",
    "    \n",
    "            for filename in os.listdir(image_folder):\n",
    "                if any(filename.lower().endswith(ext) for ext in image_extensions):\n",
    "                    image_path = os.path.join(image_folder, filename)\n",
    "                    cam_with_boxes, cam_without_boxes, salient, bounding_boxes = model(img_path=image_path)\n",
    "                    # print(layer)\n",
    "                    display_images(cam_with_boxes)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db80a52-1520-4eb9-a0e8-8393f246538d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (yolov8)",
   "language": "python",
   "name": "yolov8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
