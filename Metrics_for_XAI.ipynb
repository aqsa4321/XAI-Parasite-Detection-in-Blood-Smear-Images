{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d1f9b5-f55f-49bd-b776-9ecd5c71e626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2; print(cv2.imread(\"1.jpg\").shape[:2][::-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0e5fee-b348-401b-843f-a019214777b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "image_name = \"field0600\"\n",
    "methods = [\"EigenGradCAM\", \"GradCAM\", \"EigenCAM\", \"XGradCAM\", \"RandomCAM\", \"LayerCAM\", \"KPCA_CAM\", \"HiResCAM\", \"GradCAMPlusPlus\", \"GradCAMElementWise\"]\n",
    "layers = [\"12\", \"15\", \"17\", \"21\", \"15_17\", \"15_17_21\"]\n",
    "\n",
    "for method in methods:\n",
    "    print(f\"{method}:\")\n",
    "    for layer in layers:\n",
    "        csv_path = f'/home/axy9651/Tryp/trypanosome_parasite_detection/tryp/YOLOv8_Explainer/heatmaps/All_Layers/{method}/{layer}/{image_name}.csv'\n",
    "        if os.path.exists(csv_path):\n",
    "            # Load and process saliency map\n",
    "            saliency_map = np.loadtxt(csv_path, delimiter=',')\n",
    "            saliency_mask = (saliency_map >= 0.3).astype(int)\n",
    "            \n",
    "            # Create GT mask\n",
    "            gt_mask = np.zeros_like(saliency_mask)\n",
    "            with open(f'/home/axy9651/Tryp/trypanosome_parasite_detection/chagas_detection/DINO-atten-spa/data_cvat/val2017/labels/{image_name}.txt', 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 5:\n",
    "                        center_x, center_y, width, height = [float(x) for x in parts[1:5]]\n",
    "                        h, w = saliency_map.shape\n",
    "                        x1, y1 = int((center_x - width/2) * w), int((center_y - height/2) * h)\n",
    "                        x2, y2 = int((center_x + width/2) * w), int((center_y + height/2) * h)\n",
    "                        gt_mask[max(0,y1):min(h,y2), max(0,x1):min(w,x2)] = 1\n",
    "            \n",
    "            # Calculate IoU\n",
    "            intersection = np.sum(saliency_mask * gt_mask)\n",
    "            union = np.sum((saliency_mask + gt_mask) > 0)\n",
    "            iou = intersection / union if union > 0 else 0\n",
    "            print(f\"  Layer {layer}: {iou:.4f}\")\n",
    "        else:\n",
    "            print(f\"  Layer {layer}: Not found\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f535a0-2a30-4888-8145-2667072085c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "methods = [\"EigenGradCAM\", \"GradCAM\", \"EigenCAM\", \"XGradCAM\", \"RandomCAM\", \"LayerCAM\", \"KPCA_CAM\", \"HiResCAM\", \"GradCAMPlusPlus\", \"GradCAMElementWise\"]\n",
    "layers = [\"12\", \"15\", \"17\", \"21\", \"15_17\", \"15_17_21\"]\n",
    "labels_folder = '/home/axy9651/Tryp/trypanosome_parasite_detection/chagas_detection/DINO-atten-spa/data_cvat/val2017/labels/'\n",
    "heatmaps_base = '/home/axy9651/Tryp/trypanosome_parasite_detection/tryp/YOLOv8_Explainer/heatmaps/All_Layers'\n",
    "\n",
    "for method in methods:\n",
    "    print(f\"{method}:\")\n",
    "    for layer in layers:\n",
    "        heatmap_dir = f'{heatmaps_base}/{method}/{layer}'\n",
    "        \n",
    "        if os.path.exists(heatmap_dir):\n",
    "            layer_ious = []\n",
    "            \n",
    "            for csv_file in os.listdir(heatmap_dir):\n",
    "                if csv_file.endswith('.csv'):\n",
    "                    image_name = os.path.splitext(csv_file)[0]  # Remove .csv extension\n",
    "                    csv_path = os.path.join(heatmap_dir, csv_file)\n",
    "                    label_path = os.path.join(labels_folder, f\"{image_name}.txt\")\n",
    "                    \n",
    "                    if os.path.exists(label_path):\n",
    "                        # Load and process saliency map\n",
    "                        saliency_map = np.loadtxt(csv_path, delimiter=',')\n",
    "                        saliency_mask = (saliency_map >= 0.3).astype(int)\n",
    "                        \n",
    "                        # Create GT mask\n",
    "                        gt_mask = np.zeros_like(saliency_mask)\n",
    "                        with open(label_path, 'r') as f:\n",
    "                            for line in f:\n",
    "                                parts = line.strip().split()\n",
    "                                if len(parts) >= 5:\n",
    "                                    center_x, center_y, width, height = [float(x) for x in parts[1:5]]\n",
    "                                    h, w = saliency_map.shape\n",
    "                                    x1, y1 = int((center_x - width/2) * w), int((center_y - height/2) * h)\n",
    "                                    x2, y2 = int((center_x + width/2) * w), int((center_y + height/2) * h)\n",
    "                                    gt_mask[max(0,y1):min(h,y2), max(0,x1):min(w,x2)] = 1\n",
    "                        \n",
    "                        # Calculate IoU\n",
    "                        intersection = np.sum(saliency_mask * gt_mask)\n",
    "                        union = np.sum((saliency_mask + gt_mask) > 0)\n",
    "                        iou = intersection / union if union > 0 else 0\n",
    "                        layer_ious.append(iou)\n",
    "            \n",
    "            # Calculate average IoU for this method/layer\n",
    "            avg_iou = np.mean(layer_ious) if layer_ious else 0\n",
    "            print(f\"  Layer {layer}: {avg_iou:.4f} (images: {len(layer_ious)})\")\n",
    "        else:\n",
    "            print(f\"  Layer {layer}: Directory not found\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a768a1-6b43-4ef5-aa28-37c743e1d7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def energy_based_pointing_game(saliency_map, bbox):\n",
    "    \"\"\"\n",
    "    Calculate the EBPG score for a single or multiple bounding boxes.\n",
    "    Parameters:\n",
    "        - saliency_map: 2D-array in range [0, 1]\n",
    "        - bbox: Single bounding box (torch.Size([7])) or multiple (torch.Size([N, 7]))\n",
    "    Returns: EBPG scores for the saliency map.\n",
    "    \"\"\"\n",
    "    # Check if bbox is single or multiple\n",
    "    if len(bbox.shape) == 1:  # Single bounding box (torch.Size([7]))\n",
    "        bbox = bbox.reshape(1, -1)  # Convert to (1, N)\n",
    "\n",
    "    scores = []\n",
    "    for box in bbox:  # Iterate over all bounding boxes -> consider all detected objects in the image\n",
    "        x_min, y_min, x_max, y_max = box[:4]\n",
    "        x_min, y_min, x_max, y_max = map(lambda x: max(int(x), 0), [x_min, y_min, x_max, y_max])\n",
    "\n",
    "        # Create bounding box mask\n",
    "        mask = np.zeros_like(saliency_map)\n",
    "        mask[y_min:y_max, x_min:x_max] = 1 # y=rows, x=columns\n",
    "\n",
    "        # Normalize saliency map if needed\n",
    "        if saliency_map.max() > 1.0:\n",
    "            saliency_map = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min())\n",
    "\n",
    "        # Calculate energy\n",
    "        energy_bbox = np.sum(saliency_map * mask)\n",
    "        energy_whole = np.sum(saliency_map)\n",
    "\n",
    "        # Calculate EBPG score\n",
    "        score = energy_bbox / energy_whole if energy_whole > 0 else 0\n",
    "        scores.append(score)\n",
    "\n",
    "    return scores if len(scores) > 1 else scores[0]\n",
    "\n",
    "# Set your image name\n",
    "image_name = \"field0600\"\n",
    "\n",
    "methods = [\"EigenGradCAM\", \"GradCAM\", \"EigenCAM\", \"XGradCAM\", \"RandomCAM\", \"LayerCAM\", \"KPCA_CAM\", \"HiResCAM\", \"GradCAMPlusPlus\", \"GradCAMElementWise\"]\n",
    "layers = [\"12\", \"15\", \"17\", \"21\", \"15_17\", \"15_17_21\"]\n",
    "labels_folder = '/home/axy9651/Tryp/trypanosome_parasite_detection/chagas_detection/DINO-atten-spa/data_cvat/val2017/labels/'\n",
    "heatmaps_base = '/home/axy9651/Tryp/trypanosome_parasite_detection/tryp/YOLOv8_Explainer/heatmaps/All_Layers'\n",
    "\n",
    "# Load labels once for the image\n",
    "label_path = os.path.join(labels_folder, f\"{image_name}.txt\")\n",
    "if not os.path.exists(label_path):\n",
    "    print(f\"Label file not found for {image_name}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Processing image: {image_name}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for method in methods:\n",
    "    print(f\"{method}:\")\n",
    "    for layer in layers:\n",
    "        csv_path = f'{heatmaps_base}/{method}/{layer}/{image_name}.csv'\n",
    "        \n",
    "        if os.path.exists(csv_path):\n",
    "            # Load saliency map\n",
    "            saliency_map = np.loadtxt(csv_path, delimiter=',')\n",
    "            h, w = saliency_map.shape\n",
    "            \n",
    "            # Convert YOLO labels to bbox format [x_min, y_min, x_max, y_max]\n",
    "            bboxes = []\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 5:\n",
    "                        center_x, center_y, width, height = [float(x) for x in parts[1:5]]\n",
    "                        x_min = int((center_x - width/2) * w)\n",
    "                        y_min = int((center_y - height/2) * h)\n",
    "                        x_max = int((center_x + width/2) * w)\n",
    "                        y_max = int((center_y + height/2) * h)\n",
    "                        bboxes.append([x_min, y_min, x_max, y_max])\n",
    "                        \n",
    "            # plt.imshow(saliency_map, cmap='hot'); [plt.gca().add_patch(patches.Rectangle((bbox[0], bbox[1]), bbox[2]-bbox[0], bbox[3]-bbox[1], linewidth=2, edgecolor='cyan', facecolor='none')) for bbox in bboxes]; plt.show()\n",
    "            if bboxes:\n",
    "                bbox_array = np.array(bboxes)\n",
    "                scores = energy_based_pointing_game(saliency_map, bbox_array)\n",
    "                \n",
    "                # Handle single vs multiple scores\n",
    "                if isinstance(scores, list):\n",
    "                    avg_score = np.mean(scores)\n",
    "                    print(f\"  Layer {layer}: {avg_score:.4f} (boxes: {len(scores)})\")\n",
    "                else:\n",
    "                    print(f\"  Layer {layer}: {scores:.4f} (boxes: 1)\")\n",
    "            else:\n",
    "                print(f\"  Layer {layer}: No bounding boxes found\")\n",
    "        else:\n",
    "            print(f\"  Layer {layer}: CSV not found\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaa6024-8ab8-46a6-956f-0c7b3c269add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def energy_based_pointing_game(saliency_map, bbox):\n",
    "    \"\"\"\n",
    "    Calculate the EBPG score for a single or multiple bounding boxes.\n",
    "    Parameters:\n",
    "        - saliency_map: 2D-array in range [0, 1]\n",
    "        - bbox: Single bounding box (torch.Size([7])) or multiple (torch.Size([N, 7]))\n",
    "    Returns: EBPG scores for the saliency map.\n",
    "    \"\"\"\n",
    "    # Check if bbox is single or multiple\n",
    "    if len(bbox.shape) == 1:  # Single bounding box (torch.Size([7]))\n",
    "        bbox = bbox.reshape(1, -1)  # Convert to (1, N)\n",
    "\n",
    "    scores = []\n",
    "    for box in bbox:  # Iterate over all bounding boxes -> consider all detected objects in the image\n",
    "        x_min, y_min, x_max, y_max = box[:4]\n",
    "        x_min, y_min, x_max, y_max = map(lambda x: max(int(x), 0), [x_min, y_min, x_max, y_max])\n",
    "\n",
    "        # Create bounding box mask\n",
    "        mask = np.zeros_like(saliency_map)\n",
    "        mask[y_min:y_max, x_min:x_max] = 1 # y=rows, x=columns\n",
    "\n",
    "        # Normalize saliency map if needed\n",
    "        if saliency_map.max() > 1.0:\n",
    "            saliency_map = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min())\n",
    "\n",
    "        # Calculate energy\n",
    "        energy_bbox = np.sum(saliency_map * mask)\n",
    "        energy_whole = np.sum(saliency_map)\n",
    "\n",
    "        # Calculate EBPG score\n",
    "        score = energy_bbox / energy_whole if energy_whole > 0 else 0\n",
    "        scores.append(score)\n",
    "\n",
    "    return scores if len(scores) > 1 else scores[0]\n",
    "\n",
    "methods = [\"EigenGradCAM\", \"GradCAM\", \"EigenCAM\", \"XGradCAM\", \"RandomCAM\", \"LayerCAM\", \"KPCA_CAM\", \"HiResCAM\", \"GradCAMPlusPlus\", \"GradCAMElementWise\"]\n",
    "layers = [\"12\", \"15\", \"17\", \"21\", \"15_17\", \"15_17_21\"]\n",
    "labels_folder = '/home/axy9651/Tryp/trypanosome_parasite_detection/chagas_detection/DINO-atten-spa/data_cvat/val2017/labels/'\n",
    "heatmaps_base = '/home/axy9651/Tryp/trypanosome_parasite_detection/tryp/YOLOv8_Explainer/heatmaps/All_Layers'\n",
    "\n",
    "for method in methods:\n",
    "    print(f\"{method}:\")\n",
    "    for layer in layers:\n",
    "        heatmap_dir = f'{heatmaps_base}/{method}/{layer}'\n",
    "        \n",
    "        if os.path.exists(heatmap_dir):\n",
    "            per_image_scores = []  # Store average score per image\n",
    "            \n",
    "            for csv_file in os.listdir(heatmap_dir):\n",
    "                if csv_file.endswith('.csv'):\n",
    "                    image_name = os.path.splitext(csv_file)[0]\n",
    "                    csv_path = os.path.join(heatmap_dir, csv_file)\n",
    "                    label_path = os.path.join(labels_folder, f\"{image_name}.txt\")\n",
    "                    \n",
    "                    if os.path.exists(label_path):\n",
    "                        # Load saliency map\n",
    "                        saliency_map = np.loadtxt(csv_path, delimiter=',')\n",
    "                        h, w = saliency_map.shape\n",
    "                        \n",
    "                        # Convert YOLO labels to bbox format [x_min, y_min, x_max, y_max]\n",
    "                        bboxes = []\n",
    "                        with open(label_path, 'r') as f:\n",
    "                            for line in f:\n",
    "                                parts = line.strip().split()\n",
    "                                if len(parts) >= 5:\n",
    "                                    center_x, center_y, width, height = [float(x) for x in parts[1:5]]\n",
    "                                    x_min = int((center_x - width/2) * w)\n",
    "                                    y_min = int((center_y - height/2) * h)\n",
    "                                    x_max = int((center_x + width/2) * w)\n",
    "                                    y_max = int((center_y + height/2) * h)\n",
    "                                    bboxes.append([x_min, y_min, x_max, y_max])\n",
    "                        \n",
    "                        if bboxes:\n",
    "                            bbox_array = np.array(bboxes)\n",
    "                            scores = energy_based_pointing_game(saliency_map, bbox_array)\n",
    "                            \n",
    "                            # Calculate average score for THIS IMAGE\n",
    "                            if isinstance(scores, list):\n",
    "                                image_avg_score = np.sum(scores)\n",
    "                            else:\n",
    "                                image_avg_score = scores\n",
    "                            \n",
    "                            per_image_scores.append(image_avg_score)\n",
    "            \n",
    "            # Calculate average across all images\n",
    "            overall_avg = np.mean(per_image_scores) if per_image_scores else 0\n",
    "            print(f\"  Layer {layer}: {overall_avg:.4f} (images: {len(per_image_scores)})\")\n",
    "        else:\n",
    "            print(f\"  Layer {layer}: Directory not found\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826409c5-eeee-4bbf-ade6-39e5863d1d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import math\n",
    "import cv2\n",
    "from YOLOX.yolox.utils import postprocess\n",
    "import YOLOX.yolox.data.data_augment as data_augment\n",
    "from scipy import spatial\n",
    "from tqdm import tqdm\n",
    "\n",
    "transform = data_augment.ValTransform(legacy=False)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def auc(arr):\n",
    "    \"\"\"Returns normalized Area Under Curve of the array.\"\"\"\n",
    "    return (arr.sum() - arr[0] / 2 - arr[-1] / 2) / (arr.shape[0] - 1)  # auc formula\n",
    "\n",
    "\n",
    "def del_ins(model, img, bbox, saliency_map, mode='del', step=2000, kernel_width=0.25):\n",
    "    del_ins = np.zeros(1)\n",
    "    count = np.zeros(1)\n",
    "    HW = saliency_map.shape[1] * saliency_map.shape[2]\n",
    "    n_steps = (HW + step - 1) // step\n",
    "    for idx in tqdm(range(saliency_map.shape[0]), desc=\"DEL/INS\", leave=False):\n",
    "        target_cls = bbox[idx][-1]\n",
    "        if mode == 'del':\n",
    "            start = img.copy()\n",
    "            finish = np.zeros_like(start)\n",
    "        elif mode == 'ins':\n",
    "            start = cv2.GaussianBlur(img, (51, 51), 0)\n",
    "            finish = img.copy()\n",
    "        salient_order = np.flip(np.argsort(saliency_map[idx].reshape(HW, -1), axis=0), axis=0)\n",
    "        y = salient_order // img.shape[1]\n",
    "        x = salient_order - y * img.shape[1]\n",
    "        scores = np.zeros(n_steps + 1)\n",
    "        with torch.no_grad():\n",
    "            for i in range(n_steps + 1):\n",
    "                temp_ious = []\n",
    "                temp_score = []\n",
    "                torch_start = torch.from_numpy(start.transpose(2, 0, 1)).unsqueeze(0).float()\n",
    "                out = model(torch_start.to(device))\n",
    "                p_box, _ = postprocess(out, num_classes=1, conf_thre=0.25, nms_thre=0.45, class_agnostic=True)\n",
    "                p_box = p_box[0]\n",
    "                if p_box is None:\n",
    "                    scores[i] = 0\n",
    "                else:\n",
    "                    for b in p_box:\n",
    "                        sample_cls = b[-1]\n",
    "                        sample_box = b[:4]\n",
    "                        sample_score = b[5:-1]\n",
    "                        iou = torchvision.ops.box_iou(sample_box[:4].unsqueeze(0),\n",
    "                                                    bbox[idx][:4].unsqueeze(0)).cpu().item()\n",
    "                        distances = spatial.distance.cosine(sample_score.cpu(), bbox[idx][5:-1].cpu())\n",
    "                        weights = math.sqrt(math.exp(-(distances ** 2) / kernel_width ** 2))\n",
    "                        if target_cls != sample_cls:\n",
    "                            iou = 0\n",
    "                            sample_score = torch.tensor(0.)\n",
    "                        temp_ious.append(iou)\n",
    "                        s_score = iou * weights\n",
    "                        temp_score.append(s_score)\n",
    "                    max_score = temp_score[np.argmax(temp_ious)]\n",
    "                    scores[i] = max_score\n",
    "                x_coords = x[step * i:step * (i + 1), :]\n",
    "                y_coords = y[step * i:step * (i + 1), :]\n",
    "                start[y_coords, x_coords, :] = finish[y_coords, x_coords, :]\n",
    "        del_ins[int(target_cls)] += auc(scores)\n",
    "        count[int(target_cls)] += 1\n",
    "    return del_ins, count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337e20dc-2b57-47a8-987f-a36a6df647c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from torchvision import transforms\n",
    "\n",
    "def auc(arr):\n",
    "    return (arr.sum() - arr[0] / 2 - arr[-1] / 2) / (arr.shape[0] - 1)\n",
    "\n",
    "def process_yolo_output(results, img_shape, conf_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Process YOLOv8 output to create binary prediction mask\n",
    "    \n",
    "    Args:\n",
    "        results: YOLOv8 results object\n",
    "        img_shape: (height, width) of the image\n",
    "        conf_threshold: confidence threshold for detections\n",
    "    \n",
    "    Returns:\n",
    "        pred_mask: binary mask of predictions\n",
    "    \"\"\"\n",
    "    pred_mask = np.zeros(img_shape[:2], dtype=bool)\n",
    "    \n",
    "    if len(results) > 0 and results[0].boxes is not None:\n",
    "        boxes = results[0].boxes\n",
    "        confidences = boxes.conf.cpu().numpy()\n",
    "        xyxy = boxes.xyxy.cpu().numpy()\n",
    "        \n",
    "        # Filter by confidence\n",
    "        valid_detections = confidences >= conf_threshold\n",
    "        \n",
    "        if np.any(valid_detections):\n",
    "            valid_boxes = xyxy[valid_detections]\n",
    "            \n",
    "            for box in valid_boxes:\n",
    "                x1, y1, x2, y2 = box.astype(int)\n",
    "                # Ensure coordinates are within image bounds\n",
    "                x1 = max(0, min(x1, img_shape[1]))\n",
    "                y1 = max(0, min(y1, img_shape[0]))\n",
    "                x2 = max(0, min(x2, img_shape[1]))\n",
    "                y2 = max(0, min(y2, img_shape[0]))\n",
    "                \n",
    "                pred_mask[y1:y2, x1:x2] = True\n",
    "    \n",
    "    return pred_mask\n",
    "\n",
    "def del_ins_multiple_boxes(model_path, img_path, csv_path, label_path, \n",
    "                          mode='del', step=200, conf_threshold=0.5, device='cuda'):\n",
    "    \"\"\"\n",
    "    Compute deletion or insertion metrics for YOLOv8 model with multiple bounding boxes\n",
    "    \n",
    "    Args:\n",
    "        model_path: path to YOLOv8 model (.pt file)\n",
    "        img_path: path to input image\n",
    "        csv_path: path to saliency map CSV\n",
    "        label_path: path to ground truth labels (YOLO format)\n",
    "        mode: 'del' for deletion, 'ins' for insertion\n",
    "        step: number of pixels to modify at each step\n",
    "        conf_threshold: confidence threshold for model predictions\n",
    "        device: 'cuda' or 'cpu'\n",
    "    \n",
    "    Returns:\n",
    "        auc_score: Area under curve for the metric\n",
    "        scores: IoU scores at each step\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load model\n",
    "    model = YOLO(model_path)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Load image\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Load saliency map\n",
    "    saliency_map = np.loadtxt(csv_path, delimiter=',')\n",
    "    \n",
    "    # Resize saliency map to match image if needed\n",
    "    if saliency_map.shape != img.shape[:2]:\n",
    "        saliency_map = cv2.resize(saliency_map, (img.shape[1], img.shape[0]))\n",
    "    \n",
    "    # Create GT mask from YOLO labels\n",
    "    gt_mask = np.zeros(img.shape[:2], dtype=bool)\n",
    "    with open(label_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 5:\n",
    "                # YOLO format: class_id center_x center_y width height (normalized)\n",
    "                center_x, center_y, width, height = [float(x) for x in parts[1:5]]\n",
    "                h, w = img.shape[:2]\n",
    "                \n",
    "                # Convert to pixel coordinates\n",
    "                x1 = int((center_x - width/2) * w)\n",
    "                y1 = int((center_y - height/2) * h)\n",
    "                x2 = int((center_x + width/2) * w)\n",
    "                y2 = int((center_y + height/2) * h)\n",
    "                \n",
    "                # Ensure coordinates are within bounds\n",
    "                x1 = max(0, min(x1, w))\n",
    "                y1 = max(0, min(y1, h))\n",
    "                x2 = max(0, min(x2, w))\n",
    "                y2 = max(0, min(y2, h))\n",
    "                \n",
    "                gt_mask[y1:y2, x1:x2] = True\n",
    "    \n",
    "    # Setup for deletion/insertion\n",
    "    HW = saliency_map.size\n",
    "    n_steps = min((HW + step - 1) // step, HW // step + 1)\n",
    "    \n",
    "    if mode == 'del':\n",
    "        start = img.copy()\n",
    "        # For deletion, replace with zeros or mean color\n",
    "        baseline_value = 0  # or np.mean(img, axis=(0,1))\n",
    "    else:  # insertion\n",
    "        # For insertion, start with blurred image\n",
    "        start = cv2.GaussianBlur(img, (51, 51), 0)\n",
    "        finish = img.copy()\n",
    "    \n",
    "    # Get pixel order by saliency (most important first)\n",
    "    salient_order = np.flip(np.argsort(saliency_map.reshape(-1)))\n",
    "    y_coords = salient_order // img.shape[1]\n",
    "    x_coords = salient_order % img.shape[1]\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    # Get baseline score (before any modifications)\n",
    "    results = model(start, verbose=False)\n",
    "    pred_mask = process_yolo_output(results, img.shape, conf_threshold)\n",
    "    \n",
    "    # Calculate IoU\n",
    "    intersection = np.sum(pred_mask & gt_mask)\n",
    "    union = np.sum(pred_mask | gt_mask)\n",
    "    baseline_score = intersection / union if union > 0 else 0\n",
    "    scores.append(baseline_score)\n",
    "    \n",
    "    print(f\"Baseline IoU: {baseline_score:.4f}\")\n",
    "    print(f\"Processing {n_steps} steps...\")\n",
    "    \n",
    "    # Progressive modification\n",
    "    current_img = start.copy()\n",
    "    \n",
    "    for i in range(n_steps):\n",
    "        # Determine which pixels to modify in this step\n",
    "        start_idx = step * i\n",
    "        end_idx = min(step * (i + 1), len(salient_order))\n",
    "        \n",
    "        if start_idx >= len(salient_order):\n",
    "            break\n",
    "            \n",
    "        # Get coordinates for this batch of pixels\n",
    "        batch_y = y_coords[start_idx:end_idx]\n",
    "        batch_x = x_coords[start_idx:end_idx]\n",
    "        \n",
    "        # Modify pixels\n",
    "        if mode == 'del':\n",
    "            current_img[batch_y, batch_x, :] = baseline_value\n",
    "        else:  # insertion\n",
    "            current_img[batch_y, batch_x, :] = finish[batch_y, batch_x, :]\n",
    "        \n",
    "        # Run model on modified image\n",
    "        results = model(current_img, verbose=False)\n",
    "        pred_mask = process_yolo_output(results, img.shape, conf_threshold)\n",
    "        \n",
    "        # Calculate IoU\n",
    "        intersection = np.sum(pred_mask & gt_mask)\n",
    "        union = np.sum(pred_mask | gt_mask)\n",
    "        iou_score = intersection / union if union > 0 else 0\n",
    "        scores.append(iou_score)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Step {i+1}/{n_steps}, IoU: {iou_score:.4f}\")\n",
    "    \n",
    "    scores = np.array(scores)\n",
    "    auc_score = auc(scores)\n",
    "    \n",
    "    return auc_score, scores\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = 'best_chagas.pt'\n",
    "    img_path = '/home/axy9651/Tryp/trypanosome_parasite_detection/chagas_detection/DINO-atten-spa/data_cvat/val2017/images/field0600.jpg'\n",
    "    csv_path = '/home/axy9651/Tryp/trypanosome_parasite_detection/tryp/YOLOv8_Explainer/heatmaps/All_Layers/EigenGradCAM/12/field0600.csv'\n",
    "    label_path = '/home/axy9651/Tryp/trypanosome_parasite_detection/chagas_detection/DINO-atten-spa/data_cvat/val2017/labels/field0600.txt'\n",
    "    \n",
    "    Compute deletion metric\n",
    "    del_auc, del_scores = del_ins_multiple_boxes(\n",
    "        model_path, img_path, csv_path, label_path, \n",
    "        mode='del', step=1000, conf_threshold=0.5\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nDeletion AUC: {del_auc:.4f}\")\n",
    "    \n",
    "    # Compute insertion metric  \n",
    "    ins_auc, ins_scores = del_ins_multiple_boxes(\n",
    "        model_path, img_path, csv_path, label_path, \n",
    "        mode='ins', step=1000, conf_threshold=0.5\n",
    "    )\n",
    "    \n",
    "    print(f\"Insertion AUC: {ins_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdef178-cff6-48a1-957b-a76f809bfd45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c096aacb-921d-4218-8ed3-361da4dc732f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def auc(arr):\n",
    "    return (arr.sum() - arr[0] / 2 - arr[-1] / 2) / (arr.shape[0] - 1)\n",
    "\n",
    "def process_yolo_output(results, img_shape, conf_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Process YOLOv8 output to create binary prediction mask\n",
    "    \n",
    "    Args:\n",
    "        results: YOLOv8 results object\n",
    "        img_shape: (height, width) of the image\n",
    "        conf_threshold: confidence threshold for detections\n",
    "    \n",
    "    Returns:\n",
    "        pred_mask: binary mask of predictions\n",
    "    \"\"\"\n",
    "    pred_mask = np.zeros(img_shape[:2], dtype=bool)\n",
    "    \n",
    "    if len(results) > 0 and results[0].boxes is not None:\n",
    "        boxes = results[0].boxes\n",
    "        confidences = boxes.conf.cpu().numpy()\n",
    "        xyxy = boxes.xyxy.cpu().numpy()\n",
    "        \n",
    "        # Filter by confidence\n",
    "        valid_detections = confidences >= conf_threshold\n",
    "        \n",
    "        if np.any(valid_detections):\n",
    "            valid_boxes = xyxy[valid_detections]\n",
    "            \n",
    "            for box in valid_boxes:\n",
    "                x1, y1, x2, y2 = box.astype(int)\n",
    "                # Ensure coordinates are within image bounds\n",
    "                x1 = max(0, min(x1, img_shape[1]))\n",
    "                y1 = max(0, min(y1, img_shape[0]))\n",
    "                x2 = max(0, min(x2, img_shape[1]))\n",
    "                y2 = max(0, min(y2, img_shape[0]))\n",
    "                \n",
    "                pred_mask[y1:y2, x1:x2] = True\n",
    "    \n",
    "    return pred_mask\n",
    "\n",
    "def del_ins_multiple_boxes(model_path, img_path, csv_path, label_path, \n",
    "                          mode='del', step=200, conf_threshold=0.5, device='cuda'):\n",
    "    \"\"\"\n",
    "    Compute deletion or insertion metrics for YOLOv8 model with multiple bounding boxes\n",
    "    \n",
    "    Args:\n",
    "        model_path: path to YOLOv8 model (.pt file)\n",
    "        img_path: path to input image\n",
    "        csv_path: path to saliency map CSV\n",
    "        label_path: path to ground truth labels (YOLO format)\n",
    "        mode: 'del' for deletion, 'ins' for insertion\n",
    "        step: number of pixels to modify at each step\n",
    "        conf_threshold: confidence threshold for model predictions\n",
    "        device: 'cuda' or 'cpu'\n",
    "    \n",
    "    Returns:\n",
    "        auc_score: Area under curve for the metric\n",
    "        scores: IoU scores at each step\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load model\n",
    "    model = YOLO(model_path)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Load image\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Load saliency map\n",
    "    saliency_map = np.loadtxt(csv_path, delimiter=',')\n",
    "    \n",
    "    # Resize saliency map to match image if needed\n",
    "    if saliency_map.shape != img.shape[:2]:\n",
    "        saliency_map = cv2.resize(saliency_map, (img.shape[1], img.shape[0]))\n",
    "    \n",
    "    # Create GT mask from YOLO labels\n",
    "    gt_mask = np.zeros(img.shape[:2], dtype=bool)\n",
    "    with open(label_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 5:\n",
    "                # YOLO format: class_id center_x center_y width height (normalized)\n",
    "                center_x, center_y, width, height = [float(x) for x in parts[1:5]]\n",
    "                h, w = img.shape[:2]\n",
    "                \n",
    "                # Convert to pixel coordinates\n",
    "                x1 = int((center_x - width/2) * w)\n",
    "                y1 = int((center_y - height/2) * h)\n",
    "                x2 = int((center_x + width/2) * w)\n",
    "                y2 = int((center_y + height/2) * h)\n",
    "                \n",
    "                # Ensure coordinates are within bounds\n",
    "                x1 = max(0, min(x1, w))\n",
    "                y1 = max(0, min(y1, h))\n",
    "                x2 = max(0, min(x2, w))\n",
    "                y2 = max(0, min(y2, h))\n",
    "                \n",
    "                gt_mask[y1:y2, x1:x2] = True\n",
    "    \n",
    "    # Setup for deletion/insertion\n",
    "    HW = saliency_map.size\n",
    "    n_steps = min((HW + step - 1) // step, HW // step + 1)\n",
    "    \n",
    "    if mode == 'del':\n",
    "        start = img.copy()\n",
    "        # For deletion, replace with zeros or mean color\n",
    "        baseline_value = 0  # or np.mean(img, axis=(0,1))\n",
    "    else:  # insertion\n",
    "        # For insertion, start with blurred image\n",
    "        start = cv2.GaussianBlur(img, (51, 51), 0)\n",
    "        finish = img.copy()\n",
    "    \n",
    "    # Get pixel order by saliency (most important first)\n",
    "    salient_order = np.flip(np.argsort(saliency_map.reshape(-1)))\n",
    "    y_coords = salient_order // img.shape[1]\n",
    "    x_coords = salient_order % img.shape[1]\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    # Get baseline score (before any modifications)\n",
    "    results = model(start, verbose=False)\n",
    "    pred_mask = process_yolo_output(results, img.shape, conf_threshold)\n",
    "    \n",
    "    # Calculate IoU\n",
    "    intersection = np.sum(pred_mask & gt_mask)\n",
    "    union = np.sum(pred_mask | gt_mask)\n",
    "    baseline_score = intersection / union if union > 0 else 0\n",
    "    scores.append(baseline_score)\n",
    "    \n",
    "    print(f\"Baseline IoU: {baseline_score:.4f}\")\n",
    "    print(f\"Processing {n_steps} steps...\")\n",
    "    \n",
    "    # Progressive modification\n",
    "    current_img = start.copy()\n",
    "    \n",
    "    for i in range(n_steps):\n",
    "        # Determine which pixels to modify in this step\n",
    "        start_idx = step * i\n",
    "        end_idx = min(step * (i + 1), len(salient_order))\n",
    "        \n",
    "        if start_idx >= len(salient_order):\n",
    "            break\n",
    "            \n",
    "        # Get coordinates for this batch of pixels\n",
    "        batch_y = y_coords[start_idx:end_idx]\n",
    "        batch_x = x_coords[start_idx:end_idx]\n",
    "        \n",
    "        # Modify pixels\n",
    "        if mode == 'del':\n",
    "            current_img[batch_y, batch_x, :] = baseline_value\n",
    "        else:  # insertion\n",
    "            current_img[batch_y, batch_x, :] = finish[batch_y, batch_x, :]\n",
    "        \n",
    "        # Run model on modified image\n",
    "        results = model(current_img, verbose=False)\n",
    "        pred_mask = process_yolo_output(results, img.shape, conf_threshold)\n",
    "        \n",
    "        # Calculate IoU\n",
    "        intersection = np.sum(pred_mask & gt_mask)\n",
    "        union = np.sum(pred_mask | gt_mask)\n",
    "        iou_score = intersection / union if union > 0 else 0\n",
    "        scores.append(iou_score)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Step {i+1}/{n_steps}, IoU: {iou_score:.4f}\")\n",
    "    \n",
    "    scores = np.array(scores)\n",
    "    auc_score = auc(scores)\n",
    "    \n",
    "    return auc_score, scores\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = 'best_chagas.pt'\n",
    "    images_dir = '/home/axy9651/Tryp/trypanosome_parasite_detection/chagas_detection/DINO-atten-spa/data_cvat/val2017/images'\n",
    "    labels_dir = '/home/axy9651/Tryp/trypanosome_parasite_detection/chagas_detection/DINO-atten-spa/data_cvat/val2017/labels'\n",
    "    csv_dir = '/home/axy9651/Tryp/trypanosome_parasite_detection/tryp/YOLOv8_Explainer/heatmaps/All_Layers/EigenGradCAM/12'\n",
    "    \n",
    "    # Get all image files\n",
    "    image_files = glob.glob(os.path.join(images_dir, '*.jpg'))\n",
    "    \n",
    "    print(f\"Found {len(image_files)} images to process\\n\")\n",
    "    \n",
    "    del_aucs = []\n",
    "    ins_aucs = []\n",
    "    \n",
    "    for img_path in image_files:\n",
    "        img_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        csv_path = os.path.join(csv_dir, f\"{img_name}.csv\")\n",
    "        label_path = os.path.join(labels_dir, f\"{img_name}.txt\")\n",
    "        \n",
    "        if not os.path.exists(csv_path) or not os.path.exists(label_path):\n",
    "            print(f\"Skipping {img_name} - missing files\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Processing {img_name}...\")\n",
    "        \n",
    "        # Compute deletion metric\n",
    "        del_auc, del_scores = del_ins_multiple_boxes(\n",
    "            model_path, img_path, csv_path, label_path, \n",
    "            mode='del', step=1000, conf_threshold=0.5\n",
    "        )\n",
    "        \n",
    "        # Compute insertion metric  \n",
    "        ins_auc, ins_scores = del_ins_multiple_boxes(\n",
    "            model_path, img_path, csv_path, label_path, \n",
    "            mode='ins', step=1000, conf_threshold=0.5\n",
    "        )\n",
    "        \n",
    "        del_aucs.append(del_auc)\n",
    "        ins_aucs.append(ins_auc)\n",
    "        \n",
    "        print(f\"Deletion AUC: {del_auc:.4f}\")\n",
    "        print(f\"Insertion AUC: {ins_auc:.4f}\\n\")\n",
    "    \n",
    "    print(f\"Mean Deletion AUC: {np.mean(del_aucs):.4f}\")\n",
    "    print(f\"Mean Insertion AUC: {np.mean(ins_aucs):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1342ae9b-9bfb-42a9-bfa9-8fe7dae04fce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "# Paths\n",
    "saliency_dir = '/home/axy9651/Tryp/trypanosome_parasite_detection/chagas_detection/DINO-atten-spa/saliency_maps'\n",
    "images_dir = '/home/axy9651/Tryp/trypanosome_parasite_detection/chagas_detection/DINO-atten-spa/data_cvat/val2017/images/'\n",
    "labels_dir = '/home/axy9651/Tryp/trypanosome_parasite_detection/chagas_detection/DINO-atten-spa/data_cvat/val2017/labels/'\n",
    "\n",
    "# Get all saliency files\n",
    "saliency_files = glob.glob(os.path.join(saliency_dir, \"*.csv\"))\n",
    "\n",
    "for saliency_file in saliency_files:\n",
    "    # Get image name\n",
    "    print(saliency_file)\n",
    "    image_name_base = os.path.basename(saliency_file).replace(\"\", \"\").replace(\".csv\", \"\")\n",
    "    \n",
    "    # Load image\n",
    "    image = cv2.imread(f\"{images_dir}/{image_name_base}.jpg\")\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    img_height, img_width = image.shape[:2]\n",
    "    \n",
    "    # Load saliency map\n",
    "    saliency_map = np.loadtxt(saliency_file, delimiter=',')\n",
    "    # print(saliency_map)\n",
    "    if saliency_map.shape != (img_height, img_width):\n",
    "        saliency_map = cv2.resize(saliency_map, (img_width, img_height))\n",
    "    \n",
    "    # Load YOLO boxes\n",
    "    label_file = f\"{labels_dir}/{image_name_base}.txt\"\n",
    "    boxes = []\n",
    "    if os.path.exists(label_file):\n",
    "        with open(label_file, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 5:\n",
    "                    x_center = float(parts[1]) * img_width\n",
    "                    y_center = float(parts[2]) * img_height\n",
    "                    width = float(parts[3]) * img_width\n",
    "                    height = float(parts[4]) * img_height\n",
    "                    x1 = int(x_center - width/2)\n",
    "                    y1 = int(y_center - height/2)\n",
    "                    x2 = int(x_center + width/2)\n",
    "                    y2 = int(y_center + height/2)\n",
    "                    boxes.append((x1, y1, x2, y2))\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(image)\n",
    "    plt.imshow(saliency_map, cmap='jet', alpha=0.5)\n",
    "    \n",
    "    # Draw boxes\n",
    "    for (x1, y1, x2, y2) in boxes:\n",
    "        rect = plt.Rectangle((x1, y1), x2-x1, y2-y1, fill=False, edgecolor='lime', linewidth=2)\n",
    "        plt.gca().add_patch(rect)\n",
    "    \n",
    "    plt.title(f\"{image_name_base}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f4f660-e265-4a4d-bdaa-1934e7432546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "labels_folder = '/home/axy9651/Tryp/trypanosome_parasite_detection/chagas_detection/DINO-atten-spa/data_cvat/val2017/labels/'\n",
    "heatmap_dir = '/home/axy9651/Tryp/trypanosome_parasite_detection/chagas_detection/DINO-atten-spa/saliency_maps'\n",
    "\n",
    "        \n",
    "if os.path.exists(heatmap_dir):\n",
    "    layer_ious = []\n",
    "    \n",
    "    for csv_file in os.listdir(heatmap_dir):\n",
    "        if csv_file.endswith('.csv'):\n",
    "            image_name = os.path.splitext(csv_file)[0]  # Remove .csv extension\n",
    "            csv_path = os.path.join(heatmap_dir, csv_file)\n",
    "            label_path = os.path.join(labels_folder, f\"{image_name}.txt\")\n",
    "         \n",
    "            if os.path.exists(label_path):\n",
    "                # Load and process saliency map\n",
    "                saliency_map = np.loadtxt(csv_path, delimiter=',')\n",
    "                saliency_mask = (saliency_map >= 0.3).astype(int)\n",
    "                \n",
    "                # Create GT mask\n",
    "                gt_mask = np.zeros_like(saliency_mask)\n",
    "                with open(label_path, 'r') as f:\n",
    "                    for line in f:\n",
    "                        parts = line.strip().split()\n",
    "                        if len(parts) >= 5:\n",
    "                            center_x, center_y, width, height = [float(x) for x in parts[1:5]]\n",
    "                            h, w = saliency_map.shape\n",
    "                            x1, y1 = int((center_x - width/2) * w), int((center_y - height/2) * h)\n",
    "                            x2, y2 = int((center_x + width/2) * w), int((center_y + height/2) * h)\n",
    "                            gt_mask[max(0,y1):min(h,y2), max(0,x1):min(w,x2)] = 1\n",
    "                \n",
    "                # Calculate IoU\n",
    "                intersection = np.sum(saliency_mask * gt_mask)\n",
    "                union = np.sum((saliency_mask + gt_mask) > 0)\n",
    "                iou = intersection / union if union > 0 else 0\n",
    "                layer_ious.append(iou)\n",
    "                # plt.figure(figsize=(15, 5))\n",
    "                # plt.subplot(1,3,1); plt.imshow(saliency_map, cmap='jet'); plt.title('Saliency Map'); plt.axis('off')\n",
    "                # plt.subplot(1,3,2); plt.imshow(saliency_mask, cmap='gray'); plt.title('Saliency Mask (≥0.3)'); plt.axis('off')\n",
    "                # plt.subplot(1,3,3); plt.imshow(gt_mask, cmap='gray'); plt.title('GT Mask'); plt.axis('off'); plt.show()\n",
    "    # Calculate average IoU for this method/layer\n",
    "    avg_iou = np.mean(layer_ious) if layer_ious else 0\n",
    "    print(f\"  avg iou : {avg_iou:.4f} (images: {len(layer_ious)})\")\n",
    "else:\n",
    "    print(f\"  avg iou: Directory not found\")\n",
    "    \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2073ca06-2aea-4fa8-ad1e-d9edac8d81c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def energy_based_pointing_game(saliency_map, bbox):\n",
    "    \"\"\"\n",
    "    Calculate the EBPG score for a single or multiple bounding boxes.\n",
    "    Parameters:\n",
    "        - saliency_map: 2D-array in range [0, 1]\n",
    "        - bbox: Single bounding box (torch.Size([7])) or multiple (torch.Size([N, 7]))\n",
    "    Returns: EBPG scores for the saliency map.\n",
    "    \"\"\"\n",
    "    # Check if bbox is single or multiple\n",
    "    if len(bbox.shape) == 1:  # Single bounding box (torch.Size([7]))\n",
    "        bbox = bbox.reshape(1, -1)  # Convert to (1, N)\n",
    "\n",
    "    scores = []\n",
    "    for box in bbox:  # Iterate over all bounding boxes -> consider all detected objects in the image\n",
    "        x_min, y_min, x_max, y_max = box[:4]\n",
    "        x_min, y_min, x_max, y_max = map(lambda x: max(int(x), 0), [x_min, y_min, x_max, y_max])\n",
    "\n",
    "        # Create bounding box mask\n",
    "        mask = np.zeros_like(saliency_map)\n",
    "        mask[y_min:y_max, x_min:x_max] = 1 # y=rows, x=columns\n",
    "\n",
    "        # Normalize saliency map if needed\n",
    "        if saliency_map.max() > 1.0:\n",
    "            saliency_map = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min())\n",
    "\n",
    "        # Calculate energy\n",
    "        energy_bbox = np.sum(saliency_map * mask)\n",
    "        energy_whole = np.sum(saliency_map)\n",
    "\n",
    "        # Calculate EBPG score\n",
    "        score = energy_bbox / energy_whole if energy_whole > 0 else 0\n",
    "        scores.append(score)\n",
    "\n",
    "    return scores if len(scores) > 1 else scores[0]\n",
    "\n",
    "\n",
    "labels_folder = '/home/axy9651/Tryp/trypanosome_parasite_detection/chagas_detection/DINO-atten-spa/data_cvat/val2017/labels/'\n",
    "heatmap_dir = '/home/axy9651/Tryp/trypanosome_parasite_detection/chagas_detection/DINO-atten-spa/saliency_maps'\n",
    "\n",
    " \n",
    "if os.path.exists(heatmap_dir):\n",
    "    per_image_scores = []  # Store average score per image\n",
    "    \n",
    "    for csv_file in os.listdir(heatmap_dir):\n",
    "        if csv_file.endswith('.csv'):\n",
    "            image_name = os.path.splitext(csv_file)[0]\n",
    "            csv_path = os.path.join(heatmap_dir, csv_file)\n",
    "            label_path = os.path.join(labels_folder, f\"{image_name}.txt\")\n",
    "            \n",
    "            if os.path.exists(label_path):\n",
    "                # Load saliency map\n",
    "                saliency_map = np.loadtxt(csv_path, delimiter=',')\n",
    "                h, w = saliency_map.shape\n",
    "                \n",
    "                # Convert YOLO labels to bbox format [x_min, y_min, x_max, y_max]\n",
    "                bboxes = []\n",
    "                with open(label_path, 'r') as f:\n",
    "                    for line in f:\n",
    "                        parts = line.strip().split()\n",
    "                        if len(parts) >= 5:\n",
    "                            center_x, center_y, width, height = [float(x) for x in parts[1:5]]\n",
    "                            x_min = int((center_x - width/2) * w)\n",
    "                            y_min = int((center_y - height/2) * h)\n",
    "                            x_max = int((center_x + width/2) * w)\n",
    "                            y_max = int((center_y + height/2) * h)\n",
    "                            bboxes.append([x_min, y_min, x_max, y_max])\n",
    "                \n",
    "                if bboxes:\n",
    "                    bbox_array = np.array(bboxes)\n",
    "                    scores = energy_based_pointing_game(saliency_map, bbox_array)\n",
    "                    \n",
    "                    # Calculate average score for THIS IMAGE\n",
    "                    if isinstance(scores, list):\n",
    "                        image_avg_score = np.sum(scores)\n",
    "                    else:\n",
    "                        image_avg_score = scores\n",
    "                    \n",
    "                    per_image_scores.append(image_avg_score)\n",
    "    \n",
    "    # Calculate average across all images\n",
    "    overall_avg = np.mean(per_image_scores) if per_image_scores else 0\n",
    "    print(f\"  avg ebpg : {overall_avg:.4f} (images: {len(per_image_scores)})\")\n",
    "else:\n",
    "    print(f\"  avg ebpg : Directory not found\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3982a60d-ac29-4b8a-89f7-1b38c9657860",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import RTDETR\n",
    "# After training, load best model\n",
    "model = RTDETR('/home/axy9651/Tryp/trypanosome_parasite_detection/tryp/ultralytics/runs/detect/parasite_medical_rtdetr/weights/best.pt')\n",
    "\n",
    "# Validate\n",
    "metrics = model.val()\n",
    "print(f\"mAP50: {metrics.box.map50}\")\n",
    "print(f\"Precision: {metrics.box.mp}\")\n",
    "print(f\"Recall: {metrics.box.mr}\")\n",
    "\n",
    "# Test inference on a single image\n",
    "results = model('/home/axy9651/Tryp/trypanosome_parasite_detection/tryp/ultralytics/data_RT/images/val/field0600.jpg')\n",
    "results[0].show()  # display result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d5e246-e450-40ac-9939-8906fb9c4f1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import RTDETR\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "# Load your trained model\n",
    "model = RTDETR('/home/axy9651/Tryp/trypanosome_parasite_detection/tryp/ultralytics/runs/train/parasite_medical_rtdetr6/weights/best.pt')\n",
    "\n",
    "# Path to validation images\n",
    "val_images_path = '/home/axy9651/Tryp/trypanosome_parasite_detection/tryp/ultralytics/data_RT/images/val'\n",
    "\n",
    "# Run predictions\n",
    "results = model.predict(\n",
    "    source=val_images_path,\n",
    "    conf=0.25,\n",
    "    save=False,\n",
    "    show=False\n",
    ")\n",
    "\n",
    "print(f\"Found {len(results)} images to process\\n\")\n",
    "\n",
    "# Display each image with predictions\n",
    "for i, result in enumerate(results):\n",
    "    # Get the annotated image\n",
    "    annotated_img = result.plot()\n",
    "    annotated_img_rgb = cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Display in Jupyter\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(annotated_img_rgb)\n",
    "    \n",
    "    # Title with detection info\n",
    "    img_name = Path(result.path).name\n",
    "    detection_count = len(result.boxes) if result.boxes else 0\n",
    "    plt.title(f'Image: {img_name} | Parasites Detected: {detection_count}', fontsize=14, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detection details\n",
    "    if result.boxes is not None and len(result.boxes) > 0:\n",
    "        print(f\"Image {i+1}/{len(results)}: {img_name}\")\n",
    "        confidences = result.boxes.conf.cpu().numpy()\n",
    "        for j, conf in enumerate(confidences):\n",
    "            print(f\"  Parasite {j+1}: Confidence = {conf:.3f}\")\n",
    "        print(\"-\" * 50)\n",
    "    else:\n",
    "        print(f\"Image {i+1}/{len(results)}: {img_name} - No parasites detected\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc9f75a-45dc-4bf7-8228-92853c61709f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import RTDETR\n",
    "from pathlib import Path\n",
    "\n",
    "class RTDETRAttentionExtractor:\n",
    "    def __init__(self, model_path):\n",
    "        self.model = RTDETR(model_path)\n",
    "        self.pytorch_model = self.model.model\n",
    "        self.attention_weights = {}\n",
    "        self.spatial_shapes = {}\n",
    "        self.hooks = []\n",
    "        self._register_attention_hooks()\n",
    "    \n",
    "    def _register_attention_hooks(self):\n",
    "        \"\"\"Register hooks to capture cross-attention weights\"\"\"\n",
    "        \n",
    "        def attention_hook(name):\n",
    "            def hook(module, input, output):\n",
    "                # RT-DETR uses MultiScaleDeformableAttention\n",
    "                if len(input) >= 3:\n",
    "                    query = input[0]  # [B, N_queries, C]\n",
    "                    reference_points = input[1]  # [B, N_queries, N_levels, 2]\n",
    "                    input_flatten = input[2]  # [B, sum(H_i*W_i), C]\n",
    "                    \n",
    "                    # Store these for manual attention computation\n",
    "                    self.attention_weights[f\"{name}_query\"] = query.detach().cpu()\n",
    "                    self.attention_weights[f\"{name}_reference_points\"] = reference_points.detach().cpu()\n",
    "                    self.attention_weights[f\"{name}_input_flatten\"] = input_flatten.detach().cpu()\n",
    "                    \n",
    "                    if len(input) >= 4:\n",
    "                        spatial_shapes = input[3]  # This might be a list or tensor\n",
    "                        # Handle both list and tensor cases\n",
    "                        if isinstance(spatial_shapes, torch.Tensor):\n",
    "                            self.spatial_shapes[name] = spatial_shapes.detach().cpu()\n",
    "                        elif isinstance(spatial_shapes, list):\n",
    "                            # Convert list to tensor if possible\n",
    "                            try:\n",
    "                                self.spatial_shapes[name] = torch.tensor(spatial_shapes)\n",
    "                            except:\n",
    "                                self.spatial_shapes[name] = spatial_shapes\n",
    "                        else:\n",
    "                            self.spatial_shapes[name] = spatial_shapes\n",
    "                \n",
    "                # Also capture the output\n",
    "                if isinstance(output, torch.Tensor):\n",
    "                    self.attention_weights[f\"{name}_output\"] = output.detach().cpu()\n",
    "            \n",
    "            return hook\n",
    "        \n",
    "        # Register hooks on cross-attention layers\n",
    "        for name, module in self.pytorch_model.named_modules():\n",
    "            if 'decoder' in name and 'cross_attn' in name and name.endswith('cross_attn'):\n",
    "                handle = module.register_forward_hook(attention_hook(name))\n",
    "                self.hooks.append(handle)\n",
    "                print(f\"🎯 Registered attention hook: {name}\")\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        \"\"\"Remove all hooks\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "    \n",
    "    def extract_attention_for_explainability(self, image_path, conf_threshold=0.25):\n",
    "        \"\"\"Extract attention weights for explainability\"\"\"\n",
    "        self.attention_weights = {}\n",
    "        self.spatial_shapes = {}\n",
    "        \n",
    "        # Run inference to capture attention weights\n",
    "        results = self.model.predict(image_path, conf=conf_threshold, save=False, verbose=False)\n",
    "        \n",
    "        return results, self.attention_weights, self.spatial_shapes\n",
    "\n",
    "def compute_query_spatial_attention(query, reference_points, input_flatten, spatial_shapes):\n",
    "    \"\"\"\n",
    "    Manually compute spatial attention weights for each query\n",
    "    \"\"\"\n",
    "    B, N_q, C = query.shape\n",
    "    B, N_spatial, C = input_flatten.shape\n",
    "    \n",
    "    # Compute simple attention: query similarity with spatial features\n",
    "    query_norm = F.normalize(query, dim=-1)  # [B, N_q, C]\n",
    "    spatial_norm = F.normalize(input_flatten, dim=-1)  # [B, N_spatial, C]\n",
    "    \n",
    "    # Compute attention scores: query @ spatial_features.T\n",
    "    attention_scores = torch.bmm(query_norm, spatial_norm.transpose(1, 2))  # [B, N_q, N_spatial]\n",
    "    \n",
    "    # Apply softmax to get attention weights\n",
    "    attention_weights = F.softmax(attention_scores, dim=-1)  # [B, N_q, N_spatial]\n",
    "    \n",
    "    return attention_weights\n",
    "\n",
    "def find_best_spatial_dimensions(N_spatial):\n",
    "    \"\"\"Find the best spatial dimensions for reshaping\"\"\"\n",
    "    \n",
    "    # Try perfect squares first\n",
    "    sqrt_val = int(np.sqrt(N_spatial))\n",
    "    if sqrt_val * sqrt_val == N_spatial:\n",
    "        return sqrt_val, sqrt_val\n",
    "    \n",
    "    # Try to find factors that are close to square\n",
    "    factors = []\n",
    "    for i in range(1, int(np.sqrt(N_spatial)) + 1):\n",
    "        if N_spatial % i == 0:\n",
    "            factors.append((i, N_spatial // i))\n",
    "    \n",
    "    if factors:\n",
    "        # Choose the most square-like factors\n",
    "        factors.sort(key=lambda x: abs(x[0] - x[1]))\n",
    "        return factors[0]\n",
    "    \n",
    "    # If no exact factors, use closest square and truncate\n",
    "    return sqrt_val, sqrt_val\n",
    "\n",
    "def create_explainability_heatmaps(image_path, model_path):\n",
    "    \"\"\"Create explainability heatmaps showing where each query focuses\"\"\"\n",
    "    \n",
    "    print(\"🔍 Extracting Cross-Attention Weights for Explainability...\")\n",
    "    \n",
    "    # Initialize extractor\n",
    "    extractor = RTDETRAttentionExtractor(model_path)\n",
    "    \n",
    "    # Extract attention weights\n",
    "    results, attention_data, spatial_shapes = extractor.extract_attention_for_explainability(image_path)\n",
    "    \n",
    "    if not attention_data:\n",
    "        print(\"❌ No attention weights captured\")\n",
    "        extractor.remove_hooks()\n",
    "        return\n",
    "    \n",
    "    print(f\"✅ Captured attention data: {list(attention_data.keys())}\")\n",
    "    \n",
    "    # Load image\n",
    "    img = cv2.imread(image_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_h, img_w = img_rgb.shape[:2]\n",
    "    \n",
    "    # Check for detections\n",
    "    if results[0].boxes is None or len(results[0].boxes) == 0:\n",
    "        print(\"❌ No parasites detected for explainability\")\n",
    "        extractor.remove_hooks()\n",
    "        return\n",
    "    \n",
    "    num_detections = len(results[0].boxes)\n",
    "    boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "    confidences = results[0].boxes.conf.cpu().numpy()\n",
    "    \n",
    "    print(f\"🦠 Found {num_detections} parasites to explain\")\n",
    "    \n",
    "    # Get the last decoder layer's attention (most refined)\n",
    "    layer_names = set()\n",
    "    for key in attention_data.keys():\n",
    "        if 'query' in key:\n",
    "            layer_name = '_'.join(key.split('_')[:-1])  # Remove '_query' suffix\n",
    "            layer_names.add(layer_name)\n",
    "    \n",
    "    if not layer_names:\n",
    "        print(\"❌ Could not find suitable attention data\")\n",
    "        extractor.remove_hooks()\n",
    "        return\n",
    "    \n",
    "    # Use the last layer (highest numbered)\n",
    "    layer_name = sorted(layer_names)[-1]\n",
    "    print(f\"📊 Using layer: {layer_name}\")\n",
    "    \n",
    "    query_key = f\"{layer_name}_query\"\n",
    "    input_flatten_key = f\"{layer_name}_input_flatten\"\n",
    "    ref_points_key = f\"{layer_name}_reference_points\"\n",
    "    \n",
    "    if query_key not in attention_data or input_flatten_key not in attention_data:\n",
    "        print(f\"❌ Missing required attention components\")\n",
    "        print(f\"Available keys: {list(attention_data.keys())}\")\n",
    "        extractor.remove_hooks()\n",
    "        return\n",
    "    \n",
    "    query = attention_data[query_key]  # [B, N_queries, C]\n",
    "    input_flatten = attention_data[input_flatten_key]  # [B, N_spatial, C]\n",
    "    ref_points = attention_data.get(ref_points_key, None)\n",
    "    \n",
    "    print(f\"📊 Query shape: {query.shape}\")\n",
    "    print(f\"📊 Spatial features shape: {input_flatten.shape}\")\n",
    "    \n",
    "    # Compute attention weights\n",
    "    attention_weights = compute_query_spatial_attention(query, ref_points, input_flatten, spatial_shapes)\n",
    "    print(f\"📊 Attention weights shape: {attention_weights.shape}\")\n",
    "    \n",
    "    # Convert attention weights to spatial heatmaps\n",
    "    B, N_q, N_spatial = attention_weights.shape\n",
    "    \n",
    "    # Find the best spatial dimensions\n",
    "    spatial_h, spatial_w = find_best_spatial_dimensions(N_spatial)\n",
    "    \n",
    "    print(f\"📏 Spatial dimensions: {spatial_h} x {spatial_w} = {spatial_h * spatial_w}\")\n",
    "    print(f\"📏 Available spatial features: {N_spatial}\")\n",
    "    \n",
    "    # Handle case where dimensions don't match exactly\n",
    "    if spatial_h * spatial_w > N_spatial:\n",
    "        # Pad with zeros\n",
    "        pad_size = spatial_h * spatial_w - N_spatial\n",
    "        attention_weights_padded = F.pad(attention_weights, (0, pad_size))\n",
    "        spatial_attention = attention_weights_padded[0].view(N_q, spatial_h, spatial_w)\n",
    "        print(f\"🔧 Padded {pad_size} features to match spatial dimensions\")\n",
    "    elif spatial_h * spatial_w < N_spatial:\n",
    "        # Truncate\n",
    "        attention_weights_truncated = attention_weights[:, :, :spatial_h * spatial_w]\n",
    "        spatial_attention = attention_weights_truncated[0].view(N_q, spatial_h, spatial_w)\n",
    "        print(f\"🔧 Truncated {N_spatial - spatial_h * spatial_w} features to match spatial dimensions\")\n",
    "    else:\n",
    "        # Perfect match\n",
    "        spatial_attention = attention_weights[0].view(N_q, spatial_h, spatial_w)\n",
    "    \n",
    "    print(f\"📊 Final spatial attention shape: {spatial_attention.shape}\")\n",
    "    \n",
    "    # Find queries that likely correspond to detections\n",
    "    query_energies = torch.sum(spatial_attention.view(N_q, -1), dim=1)\n",
    "    top_query_indices = torch.topk(query_energies, k=min(num_detections, 6)).indices\n",
    "    \n",
    "    print(f\"🎯 Top query indices: {top_query_indices.tolist()}\")\n",
    "    \n",
    "    # Create explainability visualization\n",
    "    fig, axes = plt.subplots(2, min(num_detections + 1, 4), figsize=(16, 10))\n",
    "    if num_detections == 0:\n",
    "        axes = axes.reshape(2, 1)\n",
    "    \n",
    "    # Original image with detections\n",
    "    axes[0, 0].imshow(img_rgb)\n",
    "    axes[0, 0].set_title('🦠 Parasite Detections', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    # Draw bounding boxes\n",
    "    for i, (box, conf) in enumerate(zip(boxes, confidences)):\n",
    "        x1, y1, x2, y2 = box\n",
    "        rect = plt.Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                           fill=False, color='red', linewidth=2)\n",
    "        axes[0, 0].add_patch(rect)\n",
    "        axes[0, 0].text(x1, y1-5, f'P{i+1}: {conf:.2f}', \n",
    "                       color='red', fontweight='bold',\n",
    "                       bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.9))\n",
    "    \n",
    "    # Generate heatmaps for each detected parasite\n",
    "    for i in range(min(num_detections, 3)):\n",
    "        if i + 1 < axes.shape[1]:\n",
    "            query_idx = top_query_indices[i].item()\n",
    "            \n",
    "            # Get attention heatmap for this query\n",
    "            query_attention = spatial_attention[query_idx].numpy()  # [H, W]\n",
    "            \n",
    "            # Resize to image dimensions\n",
    "            heatmap = cv2.resize(query_attention, (img_w, img_h))\n",
    "            \n",
    "            # Normalize heatmap\n",
    "            if heatmap.max() > 0:\n",
    "                heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n",
    "            \n",
    "            # Show individual detection\n",
    "            axes[0, i + 1].imshow(img_rgb)\n",
    "            if i < len(boxes):\n",
    "                box = boxes[i]\n",
    "                x1, y1, x2, y2 = box\n",
    "                rect = plt.Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                   fill=False, color='red', linewidth=3)\n",
    "                axes[0, i + 1].add_patch(rect)\n",
    "            \n",
    "            axes[0, i + 1].set_title(f'Parasite {i+1}', fontsize=12, fontweight='bold')\n",
    "            axes[0, i + 1].axis('off')\n",
    "            \n",
    "            # Show attention heatmap\n",
    "            axes[1, i + 1].imshow(img_rgb, alpha=0.5)\n",
    "            im = axes[1, i + 1].imshow(heatmap, cmap='jet', alpha=0.8, vmin=0, vmax=1)\n",
    "            axes[1, i + 1].set_title(f'🎯 Query {query_idx} Attention\\n(Explainability Heatmap)', \n",
    "                                    fontsize=11, fontweight='bold')\n",
    "            axes[1, i + 1].axis('off')\n",
    "            \n",
    "            # Add colorbar\n",
    "            plt.colorbar(im, ax=axes[1, i + 1], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # Summary statistics\n",
    "    axes[1, 0].axis('off')\n",
    "    summary_text = \"🧠 EXPLAINABILITY ANALYSIS\\n\"\n",
    "    summary_text += \"=\"*35 + \"\\n\\n\"\n",
    "    summary_text += f\"📁 Image: {Path(image_path).name}\\n\"\n",
    "    summary_text += f\"🦠 Parasites: {num_detections}\\n\"\n",
    "    summary_text += f\"🔍 Total Queries: {N_q}\\n\"\n",
    "    summary_text += f\"📏 Spatial Features: {N_spatial}\\n\"\n",
    "    summary_text += f\"🗺️ Feature Map: {spatial_h}×{spatial_w}\\n\\n\"\n",
    "    \n",
    "    summary_text += \"🎯 QUERY ATTENTION ENERGY:\\n\"\n",
    "    for i, idx in enumerate(top_query_indices[:3]):\n",
    "        energy = query_energies[idx].item()\n",
    "        summary_text += f\"   Query {idx}: {energy:.2f}\\n\"\n",
    "    \n",
    "    summary_text += f\"\\n💡 INTERPRETATION:\\n\"\n",
    "    summary_text += f\"Heatmaps show WHERE each\\n\"\n",
    "    summary_text += f\"query focuses to detect\\n\"\n",
    "    summary_text += f\"parasites. Red = high\\n\"\n",
    "    summary_text += f\"attention, Blue = low\\n\"\n",
    "    summary_text += f\"attention.\"\n",
    "    \n",
    "    axes[1, 0].text(0.05, 0.95, summary_text, transform=axes[1, 0].transAxes, \n",
    "                   fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "                   bbox=dict(boxstyle='round,pad=0.5', facecolor='lightcyan', alpha=0.9))\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(num_detections + 1, axes.shape[1]):\n",
    "        if i < axes.shape[1]:\n",
    "            axes[0, i].axis('off')\n",
    "            axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'explainability_attention_{Path(image_path).stem}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✅ Explainability heatmaps generated successfully!\")\n",
    "    \n",
    "    # Clean up\n",
    "    extractor.remove_hooks()\n",
    "\n",
    "# Test the explainability visualization\n",
    "model_path = '/home/axy9651/Tryp/trypanosome_parasite_detection/tryp/ultralytics/runs/train/parasite_medical_rtdetr6/weights/best.pt'\n",
    "image_path = '/home/axy9651/Tryp/trypanosome_parasite_detection/tryp/ultralytics/data_RT/images/val/field0600.jpg'\n",
    "\n",
    "create_explainability_heatmaps(image_path, model_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea01f8d-cd4a-43c5-9f9a-d8105e488b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import RTDETR\n",
    "from pathlib import Path\n",
    "\n",
    "class RTDETRSharpAttentionExtractor:\n",
    "    def __init__(self, model_path):\n",
    "        self.model = RTDETR(model_path)\n",
    "        self.pytorch_model = self.model.model\n",
    "        self.attention_weights = {}\n",
    "        self.spatial_info = {}\n",
    "        self.hooks = []\n",
    "        self._register_hooks()\n",
    "    \n",
    "    def _register_hooks(self):\n",
    "        \"\"\"Register hooks to capture attention information\"\"\"\n",
    "        \n",
    "        def attention_hook(name):\n",
    "            def hook(module, input, output):\n",
    "                # Capture inputs to deformable attention\n",
    "                if len(input) >= 3:\n",
    "                    query = input[0]  # [B, N_queries, C]\n",
    "                    reference_points = input[1]  # [B, N_queries, n_levels, 2]\n",
    "                    input_flatten = input[2]  # [B, sum(H_i*W_i), C]\n",
    "                    \n",
    "                    # Store components\n",
    "                    self.attention_weights[f\"{name}_query\"] = query.detach().cpu()\n",
    "                    self.attention_weights[f\"{name}_input_flatten\"] = input_flatten.detach().cpu()\n",
    "                    self.attention_weights[f\"{name}_reference_points\"] = reference_points.detach().cpu()\n",
    "                    \n",
    "                    if len(input) >= 4:\n",
    "                        spatial_shapes = input[3]\n",
    "                        if isinstance(spatial_shapes, torch.Tensor):\n",
    "                            self.spatial_info[f\"{name}_shapes\"] = spatial_shapes.detach().cpu()\n",
    "                        else:\n",
    "                            self.spatial_info[f\"{name}_shapes\"] = spatial_shapes\n",
    "                \n",
    "                # Store output\n",
    "                if isinstance(output, torch.Tensor):\n",
    "                    self.attention_weights[f\"{name}_output\"] = output.detach().cpu()\n",
    "            \n",
    "            return hook\n",
    "        \n",
    "        # Register hooks on cross-attention layers\n",
    "        for name, module in self.pytorch_model.named_modules():\n",
    "            if 'decoder' in name and 'cross_attn' in name and name.endswith('cross_attn'):\n",
    "                handle = module.register_forward_hook(attention_hook(name))\n",
    "                self.hooks.append(handle)\n",
    "                print(f\"🎯 Registered hook: {name}\")\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "    \n",
    "    def extract_attention(self, image_path, conf_threshold=0.25):\n",
    "        \"\"\"Extract attention components\"\"\"\n",
    "        self.attention_weights = {}\n",
    "        self.spatial_info = {}\n",
    "        \n",
    "        results = self.model.predict(image_path, conf=conf_threshold, save=False, verbose=False)\n",
    "        \n",
    "        return results, self.attention_weights, self.spatial_info\n",
    "\n",
    "def compute_sharp_attention_maps(query, input_flatten, reference_points, spatial_shapes):\n",
    "    \"\"\"Compute sharp attention maps using reference points\"\"\"\n",
    "    \n",
    "    B, N_q, C = query.shape\n",
    "    B, N_spatial, C_spatial = input_flatten.shape\n",
    "    \n",
    "    print(f\"🔧 Computing attention: N_q={N_q}, N_spatial={N_spatial}\")\n",
    "    \n",
    "    # Normalize features\n",
    "    query_norm = F.normalize(query, dim=-1)\n",
    "    input_norm = F.normalize(input_flatten, dim=-1)\n",
    "    \n",
    "    # Compute basic attention\n",
    "    attention_scores = torch.bmm(query_norm, input_norm.transpose(1, 2))  # [B, N_q, N_spatial]\n",
    "    \n",
    "    # Use reference points to create spatial weighting\n",
    "    if reference_points.dim() >= 3:\n",
    "        # reference_points: [B, N_q, n_levels, 2] or [B, N_q, 2]\n",
    "        ref_points = reference_points.view(B, N_q, -1)  # Flatten levels\n",
    "        \n",
    "        # Estimate spatial grid size from N_spatial\n",
    "        spatial_size = int(np.sqrt(N_spatial))\n",
    "        if spatial_size * spatial_size != N_spatial:\n",
    "            # Find best factors\n",
    "            factors = []\n",
    "            for i in range(1, int(np.sqrt(N_spatial)) + 1):\n",
    "                if N_spatial % i == 0:\n",
    "                    factors.append((i, N_spatial // i))\n",
    "            if factors:\n",
    "                # Choose most square-like factors\n",
    "                factors.sort(key=lambda x: abs(x[0] - x[1]))\n",
    "                h, w = factors[0]\n",
    "            else:\n",
    "                h = w = spatial_size\n",
    "        else:\n",
    "            h = w = spatial_size\n",
    "        \n",
    "        print(f\"🗺️ Spatial grid: {h}x{w} = {h*w}, available: {N_spatial}\")\n",
    "        \n",
    "        # Create coordinate grid that matches exactly N_spatial\n",
    "        if h * w == N_spatial:\n",
    "            # Perfect match\n",
    "            y_coords = torch.linspace(0, 1, h).unsqueeze(1).repeat(1, w).flatten()\n",
    "            x_coords = torch.linspace(0, 1, w).unsqueeze(0).repeat(h, 1).flatten()\n",
    "            spatial_coords = torch.stack([x_coords, y_coords], dim=1)  # [N_spatial, 2]\n",
    "        else:\n",
    "            # Create coordinates for available spatial features\n",
    "            total_coords_needed = N_spatial\n",
    "            coords_per_level = total_coords_needed // max(1, ref_points.shape[-1] // 2)\n",
    "            \n",
    "            # Generate coordinates for the actual number of spatial features\n",
    "            x_coords = torch.linspace(0, 1, total_coords_needed)\n",
    "            y_coords = torch.linspace(0, 1, total_coords_needed)\n",
    "            spatial_coords = torch.stack([x_coords, y_coords], dim=1)  # [N_spatial, 2]\n",
    "        \n",
    "        print(f\"📍 Created spatial coordinates: {spatial_coords.shape}\")\n",
    "        \n",
    "        # Apply reference point weighting to each query\n",
    "        for q_idx in range(N_q):\n",
    "            if ref_points.shape[-1] >= 2:\n",
    "                # Get reference point for this query (use first 2 coordinates)\n",
    "                ref_point = ref_points[0, q_idx, :2]  # [2]\n",
    "                \n",
    "                # Compute distances from reference point to all spatial locations\n",
    "                distances = torch.norm(spatial_coords - ref_point.unsqueeze(0), dim=1)  # [N_spatial]\n",
    "                \n",
    "                # Create Gaussian-like weighting (sharper attention)\n",
    "                spatial_weights = torch.exp(-distances * 8)  # Moderate sharpening\n",
    "                \n",
    "                # Ensure spatial_weights matches attention_scores dimension\n",
    "                if spatial_weights.shape[0] == attention_scores.shape[2]:\n",
    "                    # Apply spatial weighting to attention scores\n",
    "                    attention_scores[0, q_idx] = attention_scores[0, q_idx] * spatial_weights\n",
    "                else:\n",
    "                    print(f\"⚠️ Size mismatch: spatial_weights {spatial_weights.shape[0]} vs attention {attention_scores.shape[2]}\")\n",
    "    \n",
    "    # Apply softmax and sharpen\n",
    "    attention_weights = F.softmax(attention_scores * 1.5, dim=-1)  # Temperature scaling for sharpness\n",
    "    \n",
    "    return attention_weights\n",
    "\n",
    "def create_dino_style_heatmaps(image_path, model_path):\n",
    "    \"\"\"Create DINO-DETR style sharp attention heatmaps\"\"\"\n",
    "    \n",
    "    print(\"🔍 Creating DINO-DETR Style Sharp Attention Maps...\")\n",
    "    \n",
    "    # Initialize extractor\n",
    "    extractor = RTDETRSharpAttentionExtractor(model_path)\n",
    "    \n",
    "    # Extract attention\n",
    "    results, attention_data, spatial_info = extractor.extract_attention(image_path)\n",
    "    \n",
    "    if not attention_data:\n",
    "        print(\"❌ No attention data captured\")\n",
    "        extractor.remove_hooks()\n",
    "        return\n",
    "    \n",
    "    print(f\"✅ Captured data: {list(attention_data.keys())}\")\n",
    "    \n",
    "    # Load image\n",
    "    img = cv2.imread(image_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_h, img_w = img_rgb.shape[:2]\n",
    "    \n",
    "    # Check detections\n",
    "    if results[0].boxes is None or len(results[0].boxes) == 0:\n",
    "        print(\"❌ No parasites detected\")\n",
    "        extractor.remove_hooks()\n",
    "        return\n",
    "    \n",
    "    num_detections = len(results[0].boxes)\n",
    "    boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "    confidences = results[0].boxes.conf.cpu().numpy()\n",
    "    \n",
    "    print(f\"🦠 Found {num_detections} parasites\")\n",
    "    \n",
    "    # Get last layer data\n",
    "    layer_names = set()\n",
    "    for key in attention_data.keys():\n",
    "        if 'query' in key:\n",
    "            layer_name = '_'.join(key.split('_')[:-1])\n",
    "            layer_names.add(layer_name)\n",
    "    \n",
    "    if not layer_names:\n",
    "        print(\"❌ No suitable attention data found\")\n",
    "        extractor.remove_hooks()\n",
    "        return\n",
    "    \n",
    "    layer_name = sorted(layer_names)[-1]\n",
    "    \n",
    "    query = attention_data[f\"{layer_name}_query\"]\n",
    "    input_flatten = attention_data[f\"{layer_name}_input_flatten\"] \n",
    "    reference_points = attention_data[f\"{layer_name}_reference_points\"]\n",
    "    spatial_shapes = spatial_info.get(f\"{layer_name}_shapes\", None)\n",
    "    \n",
    "    print(f\"📊 Query: {query.shape}\")\n",
    "    print(f\"📊 Input: {input_flatten.shape}\")\n",
    "    print(f\"📊 Ref points: {reference_points.shape}\")\n",
    "    \n",
    "    # Compute sharp attention maps\n",
    "    attention_weights = compute_sharp_attention_maps(query, input_flatten, reference_points, spatial_shapes)\n",
    "    \n",
    "    B, N_q, N_spatial = attention_weights.shape\n",
    "    \n",
    "    # Find spatial dimensions for reshaping\n",
    "    spatial_size = int(np.sqrt(N_spatial))\n",
    "    if spatial_size * spatial_size > N_spatial:\n",
    "        spatial_size = int(np.sqrt(N_spatial))\n",
    "    \n",
    "    print(f\"📏 Using spatial size: {spatial_size}x{spatial_size}\")\n",
    "    \n",
    "    # Find top queries\n",
    "    query_energies = torch.sum(attention_weights[0], dim=1)\n",
    "    top_query_indices = torch.topk(query_energies, k=min(num_detections, 6)).indices\n",
    "    \n",
    "    print(f\"🎯 Top queries: {top_query_indices.tolist()}\")\n",
    "    \n",
    "    # Create DINO-style visualization\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    fig.patch.set_facecolor('midnightblue')  # Dark blue like your example\n",
    "    \n",
    "    # Create grid layout\n",
    "    n_cols = min(num_detections + 1, 4)\n",
    "    n_rows = 2\n",
    "    \n",
    "    # Original image\n",
    "    ax_orig = plt.subplot(n_rows, n_cols, 1)\n",
    "    ax_orig.imshow(img_rgb)\n",
    "    ax_orig.set_title('🦠 Detected Parasites', color='white', fontsize=12, fontweight='bold')\n",
    "    ax_orig.axis('off')\n",
    "    \n",
    "    # Draw detections\n",
    "    for i, (box, conf) in enumerate(zip(boxes, confidences)):\n",
    "        x1, y1, x2, y2 = box\n",
    "        rect = plt.Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                           fill=False, color='cyan', linewidth=2)\n",
    "        ax_orig.add_patch(rect)\n",
    "        ax_orig.text(x1, y1-5, f'P{i+1}', color='cyan', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    # Create attention heatmaps\n",
    "    for i in range(min(num_detections, n_cols-1)):\n",
    "        if i < len(top_query_indices):\n",
    "            query_idx = top_query_indices[i].item()\n",
    "            \n",
    "            # Get attention for this query\n",
    "            query_attention = attention_weights[0, query_idx].numpy()\n",
    "            \n",
    "            # Reshape to spatial grid (handle size mismatch)\n",
    "            features_to_use = min(spatial_size * spatial_size, len(query_attention))\n",
    "            attention_subset = query_attention[:features_to_use]\n",
    "            \n",
    "            # Pad if necessary\n",
    "            if len(attention_subset) < spatial_size * spatial_size:\n",
    "                padding = spatial_size * spatial_size - len(attention_subset)\n",
    "                attention_subset = np.concatenate([attention_subset, np.zeros(padding)])\n",
    "            \n",
    "            attention_spatial = attention_subset.reshape(spatial_size, spatial_size)\n",
    "            \n",
    "            # Apply strong sharpening like DINO-DETR\n",
    "            if attention_spatial.max() > 0:\n",
    "                # Normalize\n",
    "                attention_spatial = attention_spatial / attention_spatial.max()\n",
    "                # Apply strong power for sharpening\n",
    "                attention_spatial = np.power(attention_spatial, 6)  \n",
    "                # Threshold to create sharp spots\n",
    "                threshold = 0.3\n",
    "                attention_spatial[attention_spatial < threshold] = 0\n",
    "            \n",
    "            # Resize to image size\n",
    "            attention_resized = cv2.resize(attention_spatial, (img_w, img_h), interpolation=cv2.INTER_CUBIC)\n",
    "            \n",
    "            # Top row: Individual detection\n",
    "            ax1 = plt.subplot(n_rows, n_cols, i + 2)\n",
    "            ax1.imshow(img_rgb)\n",
    "            if i < len(boxes):\n",
    "                x1, y1, x2, y2 = boxes[i]\n",
    "                rect = plt.Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                   fill=False, color='cyan', linewidth=2)\n",
    "                ax1.add_patch(rect)\n",
    "            ax1.set_title(f'Parasite {i+1}', color='white', fontsize=11)\n",
    "            ax1.axis('off')\n",
    "            \n",
    "            # Bottom row: Sharp attention like DINO\n",
    "            ax2 = plt.subplot(n_rows, n_cols, n_cols + i + 2)\n",
    "            \n",
    "            # Dark background like DINO\n",
    "            dark_img = img_rgb * 0.2\n",
    "            ax2.imshow(dark_img.astype(np.uint8))\n",
    "            \n",
    "            # Show only strong attention areas\n",
    "            if attention_resized.max() > 0:\n",
    "                # Create bright spots for high attention\n",
    "                masked_attention = np.ma.masked_where(attention_resized < 0.05, attention_resized)\n",
    "                im = ax2.imshow(masked_attention, cmap='hot', alpha=0.9, vmin=0, vmax=1)\n",
    "                \n",
    "                # Add bright star points for highest attention\n",
    "                peak_attention = attention_resized > 0.7\n",
    "                if peak_attention.any():\n",
    "                    y_coords, x_coords = np.where(peak_attention)\n",
    "                    ax2.scatter(x_coords, y_coords, c='cyan', s=30, alpha=1.0, marker='*')\n",
    "            \n",
    "            ax2.set_title(f'Query {query_idx} Focus', color='white', fontsize=10)\n",
    "            ax2.axis('off')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(num_detections + 1, n_cols * n_rows):\n",
    "        ax = plt.subplot(n_rows, n_cols, i + 1)\n",
    "        ax.set_facecolor('midnightblue')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'dino_style_attention_{Path(image_path).stem}.png', \n",
    "                dpi=300, bbox_inches='tight', facecolor='midnightblue')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✅ DINO-style sharp attention maps created!\")\n",
    "    \n",
    "    extractor.remove_hooks()\n",
    "\n",
    "# Test the DINO-style visualization\n",
    "model_path = '/home/axy9651/Tryp/trypanosome_parasite_detection/tryp/ultralytics/runs/train/parasite_medical_rtdetr6/weights/best.pt'\n",
    "image_path = '/home/axy9651/Tryp/trypanosome_parasite_detection/tryp/ultralytics/data_RT/images/val/field0600.jpg'\n",
    "\n",
    "create_dino_style_heatmaps(image_path, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f78c77e-0c62-4b30-bd39-c78ff6baa9c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (yolov8)",
   "language": "python",
   "name": "yolov8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
